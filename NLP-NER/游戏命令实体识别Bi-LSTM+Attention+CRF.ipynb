{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GROUP90_A2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Readme"
      ],
      "metadata": {
        "id": "FPC3t9FEpqDs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you would like to run the whole code, you can run it sequentially.<br>\n",
        "All logs are shown in the code output, and you can check them. <br>\n",
        "The results of our Best model and baseline are in the section 4.5. <br>\n",
        "Have fun (*Ôø£Ô∏∂Ôø£)"
      ],
      "metadata": {
        "id": "fRQ9Tnt_pwZH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 -Data Collection"
      ],
      "metadata": {
        "id": "Ed97U6C37Z9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from urllib.request import urlopen\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.featstruct import remove_variables\n",
        "from gensim.models import FastText\n",
        "import gensim.downloader as api\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import spacy \n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "\n",
        "id = '1dq5pifhJ0-kZLyHXkSgaL1wV5wjZN79h'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('training_data.csv')  \n",
        "\n",
        "id = '1RC_v8cRj4iXmAbaoJayc5agmo4pnic36'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('validation_data.csv')  \n",
        "\n",
        "id = '1-yBH-1C4eLufHajEjS06I6McqaCqyKQB'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('testing_data.csv')  \n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "training_data = pd.read_csv(\"/content/training_data.csv\")\n",
        "validation_data = pd.read_csv(\"/content/validation_data.csv\")\n",
        "testing_data = pd.read_csv(\"/content/testing_data.csv\")\n",
        "\n",
        "print(\"------------------------------------\")\n",
        "print(\"Size of training dataset: {0}\".format(len(training_data)))\n",
        "print(\"Size of validation dataset: {0}\".format(len(validation_data)))\n",
        "#print(\"Size of testing dataset: {0}\".format(len(testing_data)))\n",
        "print(\"------------------------------------\")\n",
        "\n",
        "print(\"------------------------------------\")\n",
        "print(\"Sample Data\")\n",
        "print(\"LABEL: {0} / SENTENCE: {1}\".format(validation_data.iloc[-1,0], validation_data.iloc[-1,1]))\n",
        "print(\"------------------------------------\")\n",
        "\n",
        "\n",
        "training_original = training_data.copy()\n",
        "validation_original = validation_data.copy()\n",
        "testing_original = testing_data.copy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQJfujFZ7Yud",
        "outputId": "bd065e22-2e0f-4649-d839-1764deaca5db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "------------------------------------\n",
            "Size of training dataset: 26078\n",
            "Size of validation dataset: 8705\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "Sample Data\n",
            "LABEL: i feel for you sniper / SENTENCE: P O O P C\n",
            "------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## POS and DEP lists:\n",
        "\n",
        "id = '1gQ3MtX3L6D53s3F99pxpPIAVPcXs411v'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('training_pos.txt')\n",
        "\n",
        "id = '1_5lg5LiBDkcsftXwQY_suA2liwO8bIGF'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('training_dep.txt')\n",
        "\n",
        "id = '1NMQQfo215c9LSXkbQwHE8wzqSz55O3es'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('testing_pos.txt')\n",
        "\n",
        "id = '1S4PQ-cmOAFKCWQ3ierO00jUGenc7TJoG'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('testing_dep.txt')\n",
        "\n",
        "id = '1uxLMYswQ9b7TA59s_JVt8HnjIuDiD3YG'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('validation_pos.txt')\n",
        "\n",
        "id = '1hcEpLfmjGxfsFU_T7ZghNuUxR99AaroV'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('validation_dep.txt')\n",
        "\n",
        "# turning text to lists\n",
        "def list_from_txt(text_file):\n",
        "  final_list = []\n",
        "  for line in open('{}.txt'.format(text_file),'r'):\n",
        "    line = line.split(\" \")\n",
        "    sub_list = []\n",
        "    for tag in line:\n",
        "      sub_list.append(tag.strip())\n",
        "    final_list.append(sub_list)\n",
        "  return final_list"
      ],
      "metadata": {
        "id": "ZQnaIqVZhxSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 -Preprocessing"
      ],
      "metadata": {
        "id": "-cLIZu1xXs5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# REMOVING '[SEPA]', additional whitespaces, SEPA in labels\n",
        "### DO WE DO LOWERCASE??????????\n",
        "\n",
        "# function to reduce length of consecutive characters: wowwwwwwww -> wowww\n",
        "def reduce_len(word):\n",
        "  reduced_word = ''\n",
        "  for w in word:\n",
        "    if len(reduced_word) >= 3:\n",
        "      if w==reduced_word[-3]==reduced_word[-2]==reduced_word[-1]:\n",
        "        continue\n",
        "    reduced_word += w\n",
        "  return reduced_word\n",
        "\n",
        "def preprocessing_pd(df, sentences_col, labels_col):\n",
        "  _df = df.copy()\n",
        "  _df[sentences_col] = _df[sentences_col].str.lower()\n",
        "  _df[sentences_col] = _df[sentences_col].str.replace(r' +', ' ')\n",
        "  _df[sentences_col] = _df[sentences_col].str.strip()\n",
        "  _df[labels_col] = _df[labels_col].str.strip()\n",
        "  _df[sentences_col] = _df[sentences_col].apply(lambda x: reduce_len(x))\n",
        "  _df['full_sentences'] = _df[sentences_col]\n",
        "  _df[sentences_col] = _df[sentences_col].str.split(' ')\n",
        "  _df[labels_col] = _df[labels_col].str.split(' ')\n",
        "  return _df"
      ],
      "metadata": {
        "id": "gOTMlKqRXui6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "training_data = preprocessing_pd(training_original,'sents','labels')\n",
        "validation_data = preprocessing_pd(validation_original,'sents','labels')"
      ],
      "metadata": {
        "id": "8QxhJSY-Ye7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_y_train = [[y for y in Y.split()] for Y in training_original['labels'].tolist()]\n",
        "target_y_validation = [[y for y in Y.split()] for Y in validation_original['labels'].tolist()]"
      ],
      "metadata": {
        "id": "K_KzMRbrg9i3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 -Embedding"
      ],
      "metadata": {
        "id": "Ge7SHl04-TGT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ‚ö†Ô∏è IMPORTANT Pt.1‚ö†Ô∏è FUNCTION TO CREATE MARKINGS FOR SENTENCES üëÄ \n",
        "Arguments:\n",
        "1. training dataframe (train_dat)\n",
        "2. marking type: 'pos', 'dep', 'tag' (type)\n",
        "\n",
        "given these three arguments...\n",
        "1. returns the list of sentences with markings instead of words <br>\n",
        "(returned with index 0) <br>\n",
        "e.g. data_marking(*args*) [ 0 ] = [ [ NOUN , PRN , VERB ] , [ ... ] ]\n",
        "<br>\n",
        "<br>\n",
        "2. returns the dictionary of a unique mark type to a number <br>\n",
        "(returned with index 1) <br>\n",
        "e.g. data_marking(*args*) [ 1 ] = { NOUN : 1 , PRN : 2 , VERB : 3 , ... }"
      ],
      "metadata": {
        "id": "FuFERLAmFeJr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1. Helper Functions"
      ],
      "metadata": {
        "id": "KZ_odngip84a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fixing tokenisation issues with fixed sequences\n",
        "def remerge_sent(sent):\n",
        "  i = 0\n",
        "  while i < len(sent)-1:\n",
        "      to = sent[i]\n",
        "      if not to.whitespace_:\n",
        "          nto = sent[i+1]\n",
        "          sent.merge(to.idx, nto.idx+len(nto))\n",
        "      i += 1\n",
        "  return sent"
      ],
      "metadata": {
        "id": "sQZHr7O_S4cW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate syntactic version of each sentence \n",
        "### train_dat : {training dataframe}\n",
        "### type : {'pos', 'dep', 'tag'}\n",
        "\n",
        "def data_marking(train_dat, type):\n",
        "  \n",
        "  # HELPER FUNCTION 2\n",
        "  # function to remerge tokens as spaCy splits can increase token counts e.g. cant -> ca nt\n",
        "  # retains only top most marking of split tokens\n",
        "  def remerge_sent(sent):\n",
        "    i = 0\n",
        "    while i < len(sent)-1:\n",
        "        to = sent[i]\n",
        "        if not to.whitespace_:\n",
        "            nto = sent[i+1]\n",
        "            sent.merge(to.idx, nto.idx+len(nto))\n",
        "        i += 1\n",
        "    return sent\n",
        "\n",
        "  # creating marked representation of each sentence for a designated type\n",
        "  mark_list = []\n",
        "  i = 0\n",
        "  for row in train_dat.itertuples():\n",
        "    temp_list = []\n",
        "    sent = remerge_sent(nlp(row.full_sentences))\n",
        "    if len(sent)!=len(row.sents):\n",
        "      sent = remerge_sent(sent)\n",
        "    i += 1\n",
        "    print(i)\n",
        "    if type=='pos':\n",
        "      for token in sent:\n",
        "        temp_list.append(token.pos_)\n",
        "\n",
        "    if type=='dep':\n",
        "      for token in sent:\n",
        "        temp_list.append(token.dep_)\n",
        "\n",
        "    if type=='tag':\n",
        "      for token in sent:\n",
        "        temp_list.append(token.tag_)\n",
        "\n",
        "    mark_list.append(temp_list)\n",
        "\n",
        "  return mark_list"
      ],
      "metadata": {
        "id": "AM15H4Rgn4-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ‚ö†Ô∏è IMPORTANT Pt.2‚ö†Ô∏è (more helper functions) <br>\n",
        " SOME FUNCTIONS <br>\n",
        " 1. **word_list** = list of all words in sentences <br>\n",
        " 2. **word_index_generate** = list of indices of words in word list <br>\n",
        " 3. **spelling_corrector** = tries to fix spelling if a word in the test set is OOV (looks at similar words with edit-distance = 1 in training/validation set and then changes to that word). <br>\n",
        "> for example: test set contains OOV word 'ccheese', training set contains word 'cheese' which is 1 edit-distance away from 'ccheese' ( remove 'c' ) so 'ccheese' is converted to 'cheese' in the test set. <br>\n",
        "\n",
        " 4. **preprocessing_testing** = preprocess test set using spelling_correcter (which also requires a list of training/val words) <br>"
      ],
      "metadata": {
        "id": "ucJSXe85YtJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## functions to generate:\n",
        "## word_list and word_index, as well as preprocessing testing data (since we need to use word list)\n",
        "\n",
        "def word_list_generate(sentences):\n",
        "    word_list = set()\n",
        "    word_list.add('[PAD]')\n",
        "    word_list.add('[UNKNOWN]')\n",
        "    for sent in sentences:\n",
        "        for word in sent:\n",
        "            word_list.add(word)\n",
        "    word_list = list(word_list)\n",
        "    return word_list\n",
        "\n",
        "# function for generating word index\n",
        "def word_index_generate(word_list):\n",
        "    word_index = {}\n",
        "    i = 0\n",
        "    for w in word_list:\n",
        "        word_index[w] = i\n",
        "        i += 1\n",
        "    return word_index\n",
        "\n",
        "def spelling_corrector(train_val_word_list, test_data):\n",
        "    corrections_dic = {}\n",
        "    for i, sentence in enumerate(test_data):\n",
        "      for j, test_word in enumerate(sentence):\n",
        "        if len(test_word) > 3 and test_word not in train_val_word_list and test_word not in corrections_dic:\n",
        "          for train_word in train_val_word_list:\n",
        "            if nltk.edit_distance(test_word, train_word, substitution_cost=2, transpositions=True)==1:\n",
        "              corrections_dic[test_word] = train_word\n",
        "    for n, sentence in enumerate(test_data):\n",
        "      for m, word in enumerate(sentence):\n",
        "        if word in corrections_dic:\n",
        "          test_data[n][m]=test_data[n][m].replace(word,corrections_dic[word])\n",
        "    return test_data\n",
        "\n",
        "def preprocessing_testing(df, sentences_col, train_val_word_list):\n",
        "  _df = df.copy()\n",
        "  _df[sentences_col] = _df[sentences_col].str.lower()\n",
        "  _df[sentences_col] = _df[sentences_col].str.replace(r' +', ' ')\n",
        "  _df[sentences_col] = _df[sentences_col].str.strip()\n",
        "  _df['full_sentences'] = _df['sents']\n",
        "  _df[sentences_col] = _df[sentences_col].apply(lambda x: reduce_len(x))\n",
        "  _df[sentences_col] = _df[sentences_col].str.split(' ')\n",
        "  spelling_corrector(train_val_word_list, _df[sentences_col]) ### [spelling corrector]\n",
        "  return _df\n"
      ],
      "metadata": {
        "id": "Im7LICG9oHoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing test set\n",
        "testing_data = testing_original.copy()\n",
        "train_val_sentences = pd.concat([training_data['sents'], validation_data['sents']])\n",
        "train_val_word_list = word_list_generate(train_val_sentences)\n",
        "testing_preprocessed = preprocessing_testing(testing_data, 'sents', train_val_word_list)\n",
        "testing_data = testing_preprocessed\n",
        "x_testing = testing_data['sents'].tolist()"
      ],
      "metadata": {
        "id": "QsgIeUjUIr93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example use-case of spelling_corrector\n",
        "print('original sentence:', testing_original['sents'][93])\n",
        "print('spelling corrected:', testing_data['sents'][93])\n",
        "print('STUPIUD -> stupid')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1oaJZ1Mos5q",
        "outputId": "cb7355d5-fa8d-466d-e596-3d935d20d9cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original sentence: STUPIUD PAUSE\n",
            "spelling corrected: ['stupid', 'pause']\n",
            "STUPIUD -> stupid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dict_ix(mark_list):\n",
        "  mark_dic = {}\n",
        "  for sentence in mark_list:\n",
        "    for mark in sentence:\n",
        "      if mark not in mark_dic:\n",
        "        mark_dic[mark] = len(mark_dic)\n",
        "  return mark_dic"
      ],
      "metadata": {
        "id": "fFbYxKxTPZZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2. Syntactic *Embedding*"
      ],
      "metadata": {
        "id": "PZ3zqCgPZe_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### POS tag"
      ],
      "metadata": {
        "id": "2qie3UJMJse7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_pos_marking = list_from_txt('training_pos')\n",
        "validation_pos_marking = list_from_txt('validation_pos')\n",
        "testing_pos_marking = list_from_txt('testing_pos')"
      ],
      "metadata": {
        "id": "V8m-H1T0vDZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_ix = dict_ix(training_pos_marking + validation_pos_marking + testing_pos_marking)\n",
        "pos_embedding = np.eye(len(list(pos_ix.values())))"
      ],
      "metadata": {
        "id": "OtTdZg6eVz-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dep tag"
      ],
      "metadata": {
        "id": "rOx9W3lDQRUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_dep_marking = list_from_txt('training_dep')\n",
        "validation_dep_marking = list_from_txt('validation_dep')\n",
        "testing_dep_marking = list_from_txt('testing_dep')"
      ],
      "metadata": {
        "id": "6kxDmx9iQh5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dep_ix = dict_ix(training_dep_marking + validation_dep_marking + testing_dep_marking)\n",
        "dep_embedding = np.eye(len(list(dep_ix.values())))"
      ],
      "metadata": {
        "id": "eT7qu7YHOO0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3. Semantic Embedding"
      ],
      "metadata": {
        "id": "nNVNlfiQ0jyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#glove_twitter_50 = api.load(\"glove-twitter-50\")\n",
        "ft_train_val = FastText(sentences=pd.concat([validation_data['sents'], training_data['sents']]),window=3,size=100, min_count=1)"
      ],
      "metadata": {
        "id": "OENrAoPe0ibp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4. Domain Embedding"
      ],
      "metadata": {
        "id": "INTUb0VD6HqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##### DOTA WIKI ######\n",
        "char_url = \"https://dota2.fandom.com/wiki/Heroes\"\n",
        "char_page = urlopen(char_url)\n",
        "char_html_bytes = char_page.read()\n",
        "char_html = char_html_bytes.decode(\"utf-8\")\n",
        "result = re.findall('black;\">(.*?)</span></div></div><div style=', char_html) + ['Wraith_King','Zeus','Weaver']\n",
        "char_list = []\n",
        "for char in result:\n",
        "  if ' ' in char:\n",
        "    char = char.replace(' ','_')\n",
        "  if \"'\" in char:\n",
        "    char = char.replace(\"'\",'%27')\n",
        "  char_list.append(char)\n",
        "\n",
        "# PREPROCESSING HTML PAGE TO GET RELEVANT INFORMATION\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def preprocess_html(html_):\n",
        "  sww = stopwords.words('english')\n",
        "  html = re.sub(r'<.+?>', '', html_)\n",
        "  html = re.sub(r\"\\n|\\t|\\'s|\\'\",'',html)\n",
        "  html = re.sub(r'^.*?(Gameplay\\[\\]Playstyle)','',html)\n",
        "  html = re.sub(\"\\d+\", \"\",html)\n",
        "  html = re.sub(r'\\S*\\[\\]','',html)\n",
        "  html = re.sub(\"-\",'',html)\n",
        "  html = re.sub(\"[^\\w\\s]\",' ',html)\n",
        "  html = re.sub(\"NewPP.*$\",'',html)\n",
        "  html = html.strip()\n",
        "  html = re.sub(\" +\",' ',html).lower()\n",
        "  html = html.split(' ')\n",
        "  return html\n",
        "\n",
        "# LIST OF CHAMP=SPECIFIC INFORMATION\n",
        "dota_domain_docs_sww = []\n",
        "dota_domain_docs_nosww = []\n",
        "for char in char_list:\n",
        "  url = \"https://dota2.fandom.com/wiki/{}/Guide\".format(char)\n",
        "  page = urlopen(url)\n",
        "  html_bytes = page.read()\n",
        "  html = html_bytes.decode(\"utf-8\")\n",
        "  dota_domain_docs_sww.append(preprocess_html(html))\n",
        "\n",
        "\n",
        "from gensim.models import FastText\n",
        "ft_DOTAWIKI = FastText(sentences=dota_domain_docs_sww, window=10, size=50, min_count=1)"
      ],
      "metadata": {
        "id": "XaVU2oVP08z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### CONDA TEST ######\n",
        "id = '1G_0175o6cznxpRSsbPSL73FVWchauMKs'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('CONDA_test_comments.csv') \n",
        "CONDA_domain = pd.read_csv('/content/CONDA_test_comments.csv')\n",
        "CONDA_domain['utterance'] = CONDA_domain['utterance'].str.lower()\n",
        "CONDA_domain['utterance'] = CONDA_domain['utterance'].str.replace(r' +', ' ')\n",
        "CONDA_domain['utterance'] = CONDA_domain['utterance'].str.strip()\n",
        "CONDA_domain['utterance'] = CONDA_domain['utterance'].apply(lambda x: reduce_len(x))\n",
        "CONDA_domain['utterance'] = CONDA_domain['utterance'].str.split(' ')\n",
        "\n",
        "from gensim.models import FastText\n",
        "ft_CONDA = FastText(sentences=CONDA_domain, window=3, size=100, min_count=1)"
      ],
      "metadata": {
        "id": "jr53CLNlxNBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5. Generate word_to_ix and tag_to_ix"
      ],
      "metadata": {
        "id": "wtPdJh0hRXgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_sentences = pd.concat([training_data['sents'], validation_data['sents'], testing_data['sents']])\n",
        "full_word_list = word_list_generate(full_sentences)\n",
        "full_word_to_ix = word_index_generate(full_word_list) \n",
        "print('sent_word_list:',full_word_list[:20])\n",
        "print()\n",
        "len(full_word_list)\n",
        "len(full_word_to_ix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkGIB4voWPrA",
        "outputId": "1cf5bec6-096f-45ca-c0e0-c81e2e08158f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sent_word_list: ['yea', 'divided', 'saboid', 'fckng', 'sentry', 'caught', 'drawing', 'comenden', 'tower', 'gagos', 'yeahhh', 'gamme', 'scouting', 'blyadi', 'okey', 'rim', 'morphling', 'diferent', 'nobody', 'nxt']\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10963"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tag to index\n",
        "START_TAG = \"<START>\"\n",
        "STOP_TAG = \"<STOP>\"\n",
        "tag_to_ix = {START_TAG:0, STOP_TAG:1}\n",
        "for tags in target_y_train + target_y_validation:\n",
        "    for tag in tags:\n",
        "        if tag not in tag_to_ix:\n",
        "            tag_to_ix[tag] = len(tag_to_ix)\n",
        "tag_to_ix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9JESW5fft85",
        "outputId": "849d3020-c70e-4f42-c50e-cadad27ee127"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<START>': 0,\n",
              " '<STOP>': 1,\n",
              " 'C': 8,\n",
              " 'D': 7,\n",
              " 'O': 2,\n",
              " 'P': 4,\n",
              " 'S': 6,\n",
              " 'SEPA': 5,\n",
              " 'T': 3}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### convert dataset into idxs\n",
        "\n",
        "x_train = training_data['sents'].tolist()\n",
        "x_validation = validation_data['sents'].tolist()\n",
        "x_testing = testing_data['sents'].tolist()\n",
        "\n",
        "\n",
        "def index_generate(sentences, ind):\n",
        "    ind_list = []\n",
        "    for sent in sentences:\n",
        "        ind_list.append([ind[w] for w in sent])\n",
        "    return ind_list\n",
        "\n",
        "train_input_index =  index_generate(x_train,full_word_to_ix)\n",
        "train_output_index = index_generate(target_y_train,tag_to_ix)\n",
        "val_input_index = index_generate(x_validation, full_word_to_ix)\n",
        "val_output_index = index_generate(target_y_validation,tag_to_ix)\n",
        "train_dep_index = index_generate(training_dep_marking,dep_ix)\n",
        "train_pos_index =  index_generate(training_pos_marking,pos_ix)\n",
        "val_dep_index = index_generate(validation_dep_marking,dep_ix)\n",
        "val_pos_index =  index_generate(validation_pos_marking,pos_ix)\n",
        "test_input_index = index_generate(x_testing,full_word_to_ix)\n",
        "test_dep_index = index_generate(testing_dep_marking,dep_ix)\n",
        "test_pos_index =  index_generate(testing_pos_marking,pos_ix)"
      ],
      "metadata": {
        "id": "r7gkoUVJq3aV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.6 Generate Embedding Matrix"
      ],
      "metadata": {
        "id": "N2BnKlhpRhwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "WORD_EMBEDDING_DIM_1 = 50 ## DOTA WIKI\n",
        "WORD_EMBEDDING_DIM_2 = 100 ## TRAIN / VAL\n",
        "WORD_EMBEDDING_DIM_3 = 100 ## CONDA \n",
        "WV_EMBEDDING_DIM = WORD_EMBEDDING_DIM_1 + WORD_EMBEDDING_DIM_2 + WORD_EMBEDDING_DIM_3\n",
        "embedding_matrix = []\n",
        "for word in full_word_list:\n",
        "    try:\n",
        "      a = ft_DOTAWIKI.wv[word]\n",
        "    except:\n",
        "      a = [0]*WORD_EMBEDDING_DIM_1\n",
        "    try:\n",
        "      b = ft_train_val.wv[word]\n",
        "    except:\n",
        "      b = [0]*WORD_EMBEDDING_DIM_2\n",
        "    try:\n",
        "      c = ft_CONDA.wv[word]\n",
        "    except:\n",
        "      c = [0]*WORD_EMBEDDING_DIM_3\n",
        "\n",
        "    embedding_matrix.append(np.concatenate([a,b,c]))\n",
        "embedding_matrix = np.array(embedding_matrix)\n",
        "embedding_matrix.shape\n",
        "\n",
        "embedding_dim = len(pos_ix) + len(dep_ix) + WV_EMBEDDING_DIM"
      ],
      "metadata": {
        "id": "Nd_Z7r8ZqmhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNIJQWPLcRso",
        "outputId": "a1e59a9d-a35d-416e-b34b-088ff35907ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "312"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DOTA WIKI ONLY\n",
        "WORD_EMBEDDING_DIM_1 = 50 ## DOTA WIKI\n",
        "WORD_EMBEDDING_DIM_2 = 100 ## TRAIN / VAL\n",
        "WV_EMBEDDING_DIM_domain_experiment = WORD_EMBEDDING_DIM_1 + WORD_EMBEDDING_DIM_2\n",
        "embedding_matrix_domain_experiment = []\n",
        "for word in full_word_list:\n",
        "    try:\n",
        "      a = ft_DOTAWIKI.wv[word]\n",
        "    except:\n",
        "      a = [0]*WORD_EMBEDDING_DIM_1\n",
        "    try:\n",
        "      b = ft_train_val.wv[word]\n",
        "    except:\n",
        "      b = [0]*WORD_EMBEDDING_DIM_2\n",
        "\n",
        "\n",
        "    embedding_matrix_domain_experiment.append(np.concatenate([a,b]))\n",
        "embedding_matrix_domain_experiment = np.array(embedding_matrix_domain_experiment)\n",
        "print(embedding_matrix_domain_experiment.shape)\n",
        "\n",
        "embedding_dim_domain_experiment = len(pos_ix) + len(dep_ix) + WV_EMBEDDING_DIM_domain_experiment"
      ],
      "metadata": {
        "id": "WNUBAXdhjBML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4- Ablation Experiment"
      ],
      "metadata": {
        "id": "ycU38Np0_eVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1. Ablation Study 1: Different input embedding"
      ],
      "metadata": {
        "id": "TuzvyuN4iYSQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 1"
      ],
      "metadata": {
        "id": "rbsGK5Nq5ZrV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding: Fasttext(training+validation data) + pos + dep + fasttext(dota-wiki + CONDA test)"
      ],
      "metadata": {
        "id": "CzN3FCzu6uz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "def argmax(vec):\n",
        "    # return the argmax as a python int\n",
        "    _, idx = torch.max(vec, 1)\n",
        "    return idx.item()\n",
        "\n",
        "\n",
        "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
        "def log_sum_exp(vec):\n",
        "    max_score = vec[0, argmax(vec)]\n",
        "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "    return max_score + \\\n",
        "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
        "\n",
        "class BiLSTM_CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim, wv_embedding_dim, method):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "        self.wv_embedding_dim = wv_embedding_dim\n",
        "        self.method = method\n",
        "        self.word_embeds = nn.Embedding(vocab_size, wv_embedding_dim)\n",
        "        self.word_embeds.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "        \n",
        "        self.pos_embeds = nn.Embedding(pos_embedding.shape[0], pos_embedding.shape[0])\n",
        "        self.pos_embeds.weight.data.copy_(torch.from_numpy(pos_embedding))\n",
        "        \n",
        "        self.dep_embeds = nn.Embedding(dep_embedding.shape[0], dep_embedding.shape[0])\n",
        "        self.dep_embeds.weight.data.copy_(torch.from_numpy(dep_embedding))\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
        "                            num_layers=2, bidirectional=True,dropout=0.5)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim*2, self.tagset_size)\n",
        "\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.hidden_initial()\n",
        "        self.dropout_lstm=nn.Dropout(p=0.5)\n",
        "        \n",
        "    def hidden_initial(self):\n",
        "        return (torch.randn(4, 1, self.hidden_dim // 2).to(device), ### different layers ÈúÄË¶ÅÊîπ\n",
        "                torch.randn(4, 1, self.hidden_dim // 2).to(device)) ###\n",
        "\n",
        "    def _forward_alg(self, feats):\n",
        "        # Do the forward algorithm to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "        #print('_forward_alg')\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of\n",
        "                # the previous tag\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to\n",
        "                # next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the\n",
        "                # edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the\n",
        "                # scores.\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "    # method = dot\n",
        "    def attention(self,lstm_out,hidden_out, hidden_dim, method):    \n",
        "        h_out_T = torch.transpose(hidden_out,1,2)\n",
        "        h_out_T0 = h_out_T   \n",
        "        h_out_0 = hidden_out   \n",
        "        for i in range(lstm_out.size()[0]-1):\n",
        "          h_out_T = torch.cat((h_out_T, h_out_T0), 0)\n",
        "          hidden_out = torch.cat((hidden_out, h_out_0), 0)\n",
        "        W = nn.Linear(hidden_dim,hidden_dim,bias=False)\n",
        "        if method == 'dot':\n",
        "            attention_w = F.softmax(torch.bmm(lstm_out, h_out_T),dim=-1)\n",
        "        elif method == 'scaled-dot':\n",
        "            attention_w = F.softmax(torch.bmm(lstm_out, h_out_T)/np.sqrt(self.hidden_dim),dim=-1)\n",
        "        elif method == 'cosine':\n",
        "            attention_w = F.softmax(torch.bmm(lstm_out,h_out_T)/(torch.norm(lstm_out)*torch.norm(h_out_T)),dim=-1)\n",
        "\n",
        "        elif method == 'general':\n",
        "            attention_w = F.softmax(torch.bmm(lstm_out, W(h_out_T), h_out_T),dim=-1)\n",
        "\n",
        "        attention_output = torch.bmm(attention_w, hidden_out)\n",
        "        concat_output = torch.cat((attention_output, lstm_out), 1)\n",
        "        return concat_output\n",
        "\n",
        "    def lstm_ft_get(self, sentence, pos, dep, method):\n",
        "        self.hidden = self.hidden_initial()\n",
        "        embeds = torch.cat((self.word_embeds(sentence).view(len(sentence), 1, -1),self.pos_embeds(pos).view(len(pos), 1, -1),self.dep_embeds(dep).view(len(dep), 1, -1)),2)\n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        hidden_out = torch.cat((self.hidden[0].view(2, 2, 1, -1)[1,0,:,:],self.hidden[0].view(2, 2, 1, -1)[1,1,:,:]),1) ###################‰∏çÂêålayerËøôÈáåÈúÄË¶ÅÊîπ\n",
        "        hidden_out = hidden_out.unsqueeze(0)\n",
        "        attention_out = self.attention(lstm_out,hidden_out,hidden_dim,method)\n",
        "        attention_out = attention_out.view(-1,self.hidden_dim*2)\n",
        "        attention_out = self.dropout_lstm(attention_out)\n",
        "        lstm_fts = self.hidden2tag(attention_out)\n",
        "        return lstm_fts\n",
        "\n",
        "    def sentence_scoring(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1).to(device)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long).to(device), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def viterbi_decoding(self, feats):\n",
        "        backpointers = []\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = []  # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                # We don't include the emission scores here because the max\n",
        "                # does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set\n",
        "            # of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "    def neg_ll(self, sentence, tags, pos, dep, method):\n",
        "        feats = self.lstm_ft_get(sentence, pos, dep, method)\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self.sentence_scoring(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    def forward(self, sentence, pos, dep):\n",
        "        lstm_fts = self.lstm_ft_get(sentence, pos, dep, method)\n",
        "        score, tag_seq = self.viterbi_decoding(lstm_fts)\n",
        "        return score, tag_seq\n",
        "\n",
        "\n",
        "\n",
        "def accuracy_calculation(model, input_index, output_index, pos_index, dep_index):\n",
        "  c, total = 0, 0\n",
        "  pred_list, truth = [], []\n",
        "\n",
        "  for i in range(len(input_index)):\n",
        "    sentence_in = torch.tensor(input_index[i], dtype=torch.long).to(device)\n",
        "    pos = torch.tensor(pos_index[i], dtype=torch.long).to(device)\n",
        "    dep = torch.tensor(dep_index[i], dtype=torch.long).to(device)\n",
        "    _, pred = model.forward(sentence_in, pos, dep)\n",
        "    for j in range(len(output_index[i])):\n",
        "      total += 1\n",
        "      \n",
        "      pred_list.append(pred[j])\n",
        "      truth.append(output_index[i][j])\n",
        "\n",
        "      if pred[j] == output_index[i][j]:\n",
        "        c += 1\n",
        "\n",
        "  accuracy = c/total\n",
        "\n",
        "  return truth, pred_list, accuracy\n"
      ],
      "metadata": {
        "id": "gZp_y00eaKGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "hidden_dim = 250\n",
        "\n",
        "model = BiLSTM_CRF(len(full_word_to_ix), tag_to_ix, embedding_dim, hidden_dim, WV_EMBEDDING_DIM, method='scaled-dot').to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, weight_decay=1e-4)\n",
        "\n",
        "import datetime\n",
        "for epoch in range(2):  \n",
        "    time1 = datetime.datetime.now()\n",
        "    train_loss = 0\n",
        "    method='scaled-dot'\n",
        "    model.train()\n",
        "    for i in range(len(train_input_index)):\n",
        "        #print(i)\n",
        "        tags_index = train_output_index[i]\n",
        "        dep_index = train_dep_index[i]\n",
        "        pos_index = train_pos_index[i]\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is,\n",
        "        # turn them into Tensors of word indices.\n",
        "        sentence_in = torch.tensor(train_input_index[i], dtype=torch.long).to(device)\n",
        "        pos = torch.tensor(pos_index, dtype=torch.long).to(device)\n",
        "        dep = torch.tensor(dep_index, dtype=torch.long).to(device)\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "        # Step 3. Run our forward pass.\n",
        "        loss = model.neg_ll(sentence_in, targets, pos, dep, method='scaled-dot')\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        # calling optimizer.step()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss+=loss.item()\n",
        "\n",
        "    model.eval()\n",
        "     # \n",
        "    _, _, train_acc = accuracy_calculation(model,train_input_index,train_output_index,train_pos_index,train_dep_index)\n",
        "    _, _, val_acc = accuracy_calculation(model,val_input_index,val_output_index,val_pos_index,val_dep_index)\n",
        "\n",
        "    val_loss = 0\n",
        "    for i in range(len(val_input_index)):\n",
        "        tags_index = val_output_index[i]\n",
        "        pos_index = val_pos_index[i]\n",
        "        dep_index = val_dep_index[i]\n",
        "        sentence_in = torch.tensor(val_input_index[i], dtype=torch.long).to(device)\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "        pos = torch.tensor(pos_index, dtype=torch.long).to(device)\n",
        "        dep = torch.tensor(dep_index, dtype=torch.long).to(device)\n",
        "        loss = model.neg_ll(sentence_in, targets, pos, dep, method='scaled-dot')\n",
        "        val_loss+=loss.item()\n",
        "    time2 = datetime.datetime.now()\n",
        "\n",
        "    print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))\n",
        "\n",
        "y_pred, y_true, _ = accuracy_calculation(model,val_input_index,val_output_index,val_pos_index,val_dep_index)\n",
        "def decode_output(output_list):\n",
        "    ix_to_tag = {v:k for k,v in tag_to_ix.items()}\n",
        "    return [ix_to_tag[output] for output in output_list]\n",
        "y_true_decode = decode_output(y_true)\n",
        "y_pred_decode = decode_output(y_pred)\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true_decode,y_pred_decode,digits=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12de2a40-d400-4a95-84b6-f4074541029b",
        "id": "Y9IWFxOmaKGP"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:1, Training loss: 34709.38, train acc: 0.9865, val loss: 2175.60, val acc: 0.9844, time: 630.77s\n",
            "Epoch:2, Training loss: 5511.21, train acc: 0.9958, val loss: 1519.26, val acc: 0.9933, time: 619.77s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9677    0.9802    0.9739      1620\n",
            "           D     0.9045    0.9863    0.9436       365\n",
            "           O     0.9997    0.9921    0.9959     19130\n",
            "           P     0.9970    0.9997    0.9983      3925\n",
            "           S     0.9831    0.9973    0.9901      3275\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.9598    0.9819    0.9707      1436\n",
            "\n",
            "    accuracy                         0.9933     33354\n",
            "   macro avg     0.9731    0.9911    0.9818     33354\n",
            "weighted avg     0.9935    0.9933    0.9933     33354\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.metrics import f1_score\n",
        "sklearn.metrics.f1_score(y_true_decode, y_pred_decode,average='micro')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ce552a6-27ba-4e4f-d5aa-9099bcb9af19",
        "id": "F8oxqq4yaKGP"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9932841638184325"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 2"
      ],
      "metadata": {
        "id": "KIlUqgFI62-t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding: Fasttext(training+validation data) +  fasttext(dota-wiki + CONDA test)  <br>"
      ],
      "metadata": {
        "id": "cnl5G33h7XDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1)\n",
        "embedding_dim = 250\n",
        "\n",
        "def argmax(vec):\n",
        "    # return the argmax as a python int\n",
        "    _, idx = torch.max(vec, 1)\n",
        "    return idx.item()\n",
        "\n",
        "\n",
        "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
        "def log_sum_exp(vec):\n",
        "    max_score = vec[0, argmax(vec)]\n",
        "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "    return max_score + \\\n",
        "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
        "\n",
        "class BiLSTM_CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim, wv_embedding_dim, method):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "        self.wv_embedding_dim = wv_embedding_dim\n",
        "        self.method = method\n",
        "        self.word_embeds = nn.Embedding(vocab_size, wv_embedding_dim)\n",
        "        self.word_embeds.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
        "                            num_layers=2, bidirectional=True,dropout=0.5)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim*2, self.tagset_size)\n",
        "\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.hidden_initial()\n",
        "        self.dropout_lstm=nn.Dropout(p=0.5)\n",
        "        \n",
        "    def hidden_initial(self):\n",
        "        return (torch.randn(4, 1, self.hidden_dim // 2).to(device), ### different layers ÈúÄË¶ÅÊîπ\n",
        "                torch.randn(4, 1, self.hidden_dim // 2).to(device)) ###\n",
        "\n",
        "    def _forward_alg(self, feats):\n",
        "        # Do the forward algorithm to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "        #print('_forward_alg')\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of\n",
        "                # the previous tag\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to\n",
        "                # next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the\n",
        "                # edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the\n",
        "                # scores.\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "    # method = dot\n",
        "    def attention(self,lstm_out,hidden_out, hidden_dim, method):\n",
        "        h_out_T = torch.transpose(hidden_out,1,2)\n",
        "        \n",
        "        h_out_T_1 = h_out_T   \n",
        "        h_out_0 = hidden_out   \n",
        "        for i in range(lstm_out.size()[0]-1):\n",
        "          h_out_T = torch.cat((h_out_T, h_out_T_1), 0)\n",
        "          hidden_out = torch.cat((hidden_out, h_out_0), 0)\n",
        "        W = nn.Linear(hidden_dim,hidden_dim,bias=False)\n",
        "        if method == 'dot':\n",
        "            attention_w = F.softmax(torch.bmm(lstm_out, h_out_T),dim=-1)\n",
        "        elif method == 'scaled-dot':\n",
        "            attention_w = F.softmax(torch.bmm(lstm_out, h_out_T)/np.sqrt(self.hidden_dim),dim=-1)\n",
        "        elif method == 'cosine':\n",
        "            attention_w = F.softmax(torch.bmm(lstm_out,h_out_T)/(torch.norm(lstm_out)*torch.norm(h_out_T)),dim=-1)\n",
        "\n",
        "        elif method == 'general':\n",
        "            attention_w = F.softmax(torch.bmm(lstm_out, W(h_out_T), h_out_T),dim=-1)\n",
        "\n",
        "        attention_output = torch.bmm(attention_w, hidden_out)\n",
        "        concat_output = torch.cat((attention_output, lstm_out), 1)\n",
        "        return concat_output\n",
        "\n",
        "    def lstm_ft_get(self, sentence, method):\n",
        "        self.hidden = self.hidden_initial()\n",
        "\n",
        "        w_embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
        "        lstm_out, self.hidden = self.lstm(w_embeds, self.hidden)\n",
        "\n",
        "        hidden_out = torch.cat((self.hidden[0].view(2, 2, 1, -1)[1,0,:,:],self.hidden[0].view(2, 2, 1, -1)[1,1,:,:]),1) ###################‰∏çÂêålayerËøôÈáåÈúÄË¶ÅÊîπ\n",
        "        hidden_out = hidden_out.unsqueeze(0)\n",
        "        attention_out = self.attention(lstm_out,hidden_out,hidden_dim,method)\n",
        "        attention_out = attention_out.view(-1,self.hidden_dim*2)\n",
        "        attention_out = self.dropout_lstm(attention_out)\n",
        "        lstm_fts = self.hidden2tag(attention_out)\n",
        "        return lstm_fts\n",
        "\n",
        "    def sentence_scoring(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1).to(device)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long).to(device), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + \\\n",
        "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def viterbi_decoding(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = []  # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                # We don't include the emission scores here because the max\n",
        "                # does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set\n",
        "            # of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "    def neg_ll(self, sentence, tags, method):\n",
        "        feats = self.lstm_ft_get(sentence, method)\n",
        "        #print('neg_ll')\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self.sentence_scoring(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    #def forward(self, sentence, ent, pos, dep, bert):  # dont confuse this with _forward_alg above.\n",
        "    def forward(self, sentence):\n",
        "        #print('forward')\n",
        "        # Get the emission scores from the BiLSTM\n",
        "\n",
        "        lstm_fts = self.lstm_ft_get(sentence, method)\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self.viterbi_decoding(lstm_fts)\n",
        "        return score, tag_seq\n",
        "\n",
        "\n",
        "\n",
        "def accuracy_calculation(model, input_index, output_index):\n",
        "\n",
        "  c, total = 0, 0\n",
        "\n",
        "  pred_list, truth = [], []\n",
        "\n",
        "  #print('accuracy_calculation')\n",
        "  for i in range(len(input_index)):\n",
        "    sentence_in = torch.tensor(input_index[i], dtype=torch.long).to(device)\n",
        "\n",
        "    _, pred = model.forward(sentence_in)\n",
        "    for j in range(len(output_index[i])):\n",
        "      total += 1\n",
        "      \n",
        "      pred_list.append(pred[j])\n",
        "      truth.append(output_index[i][j])\n",
        "\n",
        "      if pred[j] == output_index[i][j]:\n",
        "        c += 1\n",
        "\n",
        "  accuracy = c/total\n",
        "\n",
        "  return truth, pred_list, accuracy\n"
      ],
      "metadata": {
        "id": "RZ17DB_hDD7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "hidden_dim = 250\n",
        "\n",
        "model = BiLSTM_CRF(len(full_word_to_ix), tag_to_ix, embedding_dim, hidden_dim, WV_EMBEDDING_DIM, method='scaled-dot').to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, weight_decay=1e-4)\n",
        "\n",
        "import datetime\n",
        "for epoch in range(2):  \n",
        "    time1 = datetime.datetime.now()\n",
        "    train_loss = 0\n",
        "    method='scaled-dot'\n",
        "    model.train()\n",
        "    for i in range(len(train_input_index)):\n",
        "        #print(i)\n",
        "        tags_index = train_output_index[i]\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is,\n",
        "        # turn them into Tensors of word indices.\n",
        "        sentence_in = torch.tensor(train_input_index[i], dtype=torch.long).to(device)\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "        # Step 3. Run our forward pass.\n",
        "        loss = model.neg_ll(sentence_in, targets, method='scaled-dot')\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        # calling optimizer.step()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss+=loss.item()\n",
        "\n",
        "    model.eval()\n",
        "     # \n",
        "    _, _, train_acc = accuracy_calculation(model,train_input_index,train_output_index)\n",
        "    _, _, val_acc = accuracy_calculation(model,val_input_index,val_output_index)\n",
        "\n",
        "    val_loss = 0\n",
        "    for i in range(len(val_input_index)):\n",
        "        tags_index = val_output_index[i]\n",
        "        pos_index = val_pos_index[i]\n",
        "        dep_index = val_dep_index[i]\n",
        "        sentence_in = torch.tensor(val_input_index[i], dtype=torch.long).to(device)\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "        pos = torch.tensor(pos_index, dtype=torch.long).to(device)\n",
        "        dep = torch.tensor(dep_index, dtype=torch.long).to(device)\n",
        "        loss = model.neg_ll(sentence_in, targets, method='scaled-dot')\n",
        "        val_loss+=loss.item()\n",
        "    time2 = datetime.datetime.now()\n",
        "\n",
        "    print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))\n",
        "\n",
        "y_pred, y_true, _ = accuracy_calculation(model,val_input_index,val_output_index)\n",
        "def decode_output(output_list):\n",
        "    ix_to_tag = {v:k for k,v in tag_to_ix.items()}\n",
        "    return [ix_to_tag[output] for output in output_list]\n",
        "y_true_decode = decode_output(y_true)\n",
        "y_pred_decode = decode_output(y_pred)\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true_decode,y_pred_decode,digits=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e070aba1-7287-454c-f5c7-0a6d415be7d1",
        "id": "H-F3ZyYVDD7v"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:1, Training loss: 37623.89, train acc: 0.9850, val loss: 2187.91, val acc: 0.9835, time: 632.68s\n",
            "Epoch:2, Training loss: 5299.76, train acc: 0.9959, val loss: 1394.35, val acc: 0.9923, time: 634.36s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9586    0.9868    0.9725      1594\n",
            "           D     0.9322    0.9322    0.9322       398\n",
            "           O     0.9992    0.9915    0.9954     19132\n",
            "           P     0.9962    0.9995    0.9978      3923\n",
            "           S     0.9801    0.9985    0.9892      3261\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.9585    0.9757    0.9670      1443\n",
            "\n",
            "    accuracy                         0.9924     33354\n",
            "   macro avg     0.9750    0.9835    0.9792     33354\n",
            "weighted avg     0.9926    0.9924    0.9925     33354\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.metrics import f1_score\n",
        "sklearn.metrics.f1_score(y_true_decode, y_pred_decode,average='micro')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "760f0f11-b31d-47fe-8ad1-7f4e063ad330",
        "id": "EjNzm5HmDD7v"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9924446842957366"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 3"
      ],
      "metadata": {
        "id": "cKkYfcu-jM1v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding: Fasttext(training+validation data) + pos + dep + fasttext(dota-wiki)"
      ],
      "metadata": {
        "id": "rwHDdjPQ07Ui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1)\n",
        "\n",
        "def argmax(vec):\n",
        "    # return the argmax as a python int\n",
        "    _, idx = torch.max(vec, 1)\n",
        "    return idx.item()\n",
        "\n",
        "\n",
        "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
        "def log_sum_exp(vec):\n",
        "    max_score = vec[0, argmax(vec)]\n",
        "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "    return max_score + \\\n",
        "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
        "\n",
        "class BiLSTM_CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim, wv_embedding_dim, method):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "        self.wv_embedding_dim = wv_embedding_dim\n",
        "        self.method = method\n",
        "        self.word_embeds = nn.Embedding(vocab_size, wv_embedding_dim)\n",
        "        self.word_embeds.weight.data.copy_(torch.from_numpy(embedding_matrix_domain_experiment))\n",
        "        self.pos_embeds = nn.Embedding(pos_embedding.shape[0], pos_embedding.shape[0])\n",
        "        self.pos_embeds.weight.data.copy_(torch.from_numpy(pos_embedding))\n",
        "        self.dep_embeds = nn.Embedding(dep_embedding.shape[0], dep_embedding.shape[0])\n",
        "        self.dep_embeds.weight.data.copy_(torch.from_numpy(dep_embedding))\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
        "                            num_layers=2, bidirectional=True,dropout=0.5)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim*2, self.tagset_size)\n",
        "\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.hidden_initial()\n",
        "        self.dropout_lstm=nn.Dropout(p=0.5)\n",
        "        \n",
        "    def hidden_initial(self):\n",
        "        return (torch.randn(4, 1, self.hidden_dim // 2).to(device), ### different layers ÈúÄË¶ÅÊîπ\n",
        "                torch.randn(4, 1, self.hidden_dim // 2).to(device)) ###\n",
        "\n",
        "    def _forward_alg(self, feats):\n",
        "        # Do the forward algorithm to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "        #print('_forward_alg')\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of\n",
        "                # the previous tag\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to\n",
        "                # next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the\n",
        "                # edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the\n",
        "                # scores.\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "    def attention(self,lstm_out,hidden_out, hidden_dim, method):\n",
        "        h_out_T = torch.transpose(hidden_out,1,2)\n",
        "        h_out_T_1 = h_out_T   \n",
        "        h_out_0 = hidden_out   \n",
        "        for i in range(lstm_out.size()[0]-1):\n",
        "          h_out_T = torch.cat((h_out_T, h_out_T_1), 0)\n",
        "          hidden_out = torch.cat((hidden_out, h_out_0), 0)\n",
        "        W = nn.Linear(hidden_dim,hidden_dim,bias=False)\n",
        "        if method == 'dot':\n",
        "            attention_w = F.softmax(torch.bmm(lstm_out, h_out_T),dim=-1)\n",
        "        elif method == 'scaled-dot':\n",
        "            attention_w = F.softmax(torch.bmm(lstm_out, h_out_T)/np.sqrt(self.hidden_dim),dim=-1)\n",
        "        elif method == 'cosine':\n",
        "            attention_w = F.softmax(torch.bmm(lstm_out,h_out_T)/(torch.norm(lstm_out)*torch.norm(h_out_T)),dim=-1)\n",
        "\n",
        "        elif method == 'general':\n",
        "            attention_w = F.softmax(torch.bmm(lstm_out, W(h_out_T), h_out_T),dim=-1)\n",
        "\n",
        "        attention_output = torch.bmm(attention_w, hidden_out)\n",
        "        concat_output = torch.cat((attention_output, lstm_out), 1)\n",
        "        return concat_output\n",
        "\n",
        "    def lstm_ft_get(self, sentence, pos, dep, method):\n",
        "        self.hidden = self.hidden_initial()\n",
        "        embeds = torch.cat((self.word_embeds(sentence).view(len(sentence), 1, -1),self.pos_embeds(pos).view(len(pos), 1, -1),self.dep_embeds(dep).view(len(dep), 1, -1)),2)\n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        hidden_out = torch.cat((self.hidden[0].view(2, 2, 1, -1)[1,0,:,:],self.hidden[0].view(2, 2, 1, -1)[1,1,:,:]),1) ###################‰∏çÂêålayerËøôÈáåÈúÄË¶ÅÊîπ\n",
        "        hidden_out = hidden_out.unsqueeze(0)\n",
        "        attention_out = self.attention(lstm_out,hidden_out,hidden_dim,method)\n",
        "        attention_out = attention_out.view(-1,self.hidden_dim*2)\n",
        "        attention_out = self.dropout_lstm(attention_out)\n",
        "        lstm_fts = self.hidden2tag(attention_out)\n",
        "        return lstm_fts\n",
        "\n",
        "    def sentence_scoring(self, feats, tags):\n",
        "        score = torch.zeros(1).to(device)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long).to(device), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def negative_ll(self, sentence, tags, pos, dep, method):\n",
        "        feats = self.lstm_ft_get(sentence, pos, dep, method)\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self.sentence_scoring(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    def viterbi_decoding(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = []  # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                # We don't include the emission scores here because the max\n",
        "                # does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set\n",
        "            # of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "    def forward(self, sentence, pos, dep):\n",
        "        #print('forward')\n",
        "        # Get the emission scores from the BiLSTM\n",
        "\n",
        "        lstm_fts = self.lstm_ft_get(sentence, pos, dep, method)\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self.viterbi_decoding(lstm_fts)\n",
        "        return score, tag_seq\n",
        "\n",
        "\n",
        "\n",
        "def accuracy_calculation(model, input_index, output_index, pos_index, dep_index):\n",
        "  c, total = 0, 0\n",
        "\n",
        "  pred_list, truth = [], []\n",
        "\n",
        "  #print('accuracy_calculation')\n",
        "  for i in range(len(input_index)):\n",
        "    sentence_in = torch.tensor(input_index[i], dtype=torch.long).to(device)\n",
        "    pos = torch.tensor(pos_index[i], dtype=torch.long).to(device)\n",
        "    dep = torch.tensor(dep_index[i], dtype=torch.long).to(device)\n",
        "\n",
        "\n",
        "    _, pred = model.forward(sentence_in, pos, dep)\n",
        "    for j in range(len(output_index[i])):\n",
        "      total += 1\n",
        "      \n",
        "      pred_list.append(pred[j])\n",
        "      truth.append(output_index[i][j])\n",
        "\n",
        "      if pred[j] == output_index[i][j]:\n",
        "        c += 1\n",
        "\n",
        "  accuracy = c/total\n",
        "\n",
        "  return truth, pred_list, accuracy"
      ],
      "metadata": {
        "id": "rH8DrbgNkJms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "hidden_dim = 250\n",
        "\n",
        "model = BiLSTM_CRF(len(full_word_to_ix), tag_to_ix, embedding_dim_domain_experiment, hidden_dim, WV_EMBEDDING_DIM_domain_experiment, method='scaled-dot').to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, weight_decay=1e-4)\n",
        "\n",
        "import datetime\n",
        "for epoch in range(2):  \n",
        "    time1 = datetime.datetime.now()\n",
        "    train_loss = 0\n",
        "    method='scaled-dot'\n",
        "    model.train()\n",
        "    for i in range(len(train_input_index)):\n",
        "        #print(i)\n",
        "        tags_index = train_output_index[i]\n",
        "        dep_index = train_dep_index[i]\n",
        "        pos_index = train_pos_index[i]\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is,\n",
        "        # turn them into Tensors of word indices.\n",
        "        sentence_in = torch.tensor(train_input_index[i], dtype=torch.long).to(device)\n",
        "        pos = torch.tensor(pos_index, dtype=torch.long).to(device)\n",
        "        dep = torch.tensor(dep_index, dtype=torch.long).to(device)\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "        # Step 3. Run our forward pass.\n",
        "        loss = model.neg_ll(sentence_in, targets, pos, dep, method='scaled-dot')\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        # calling optimizer.step()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss+=loss.item()\n",
        "\n",
        "    model.eval()\n",
        "     # \n",
        "    _, _, train_acc = accuracy_calculation(model,train_input_index,train_output_index,train_pos_index,train_dep_index)\n",
        "    _, _, val_acc = accuracy_calculation(model,val_input_index,val_output_index,val_pos_index,val_dep_index)\n",
        "\n",
        "    val_loss = 0\n",
        "    for i in range(len(val_input_index)):\n",
        "        tags_index = val_output_index[i]\n",
        "        pos_index = val_pos_index[i]\n",
        "        dep_index = val_dep_index[i]\n",
        "        sentence_in = torch.tensor(val_input_index[i], dtype=torch.long).to(device)\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "        pos = torch.tensor(pos_index, dtype=torch.long).to(device)\n",
        "        dep = torch.tensor(dep_index, dtype=torch.long).to(device)\n",
        "        loss = model.neg_ll(sentence_in, targets, pos, dep, method='scaled-dot')\n",
        "        val_loss+=loss.item()\n",
        "    time2 = datetime.datetime.now()\n",
        "\n",
        "    print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))\n",
        "\n",
        "y_pred, y_true, _ = accuracy_calculation(model,val_input_index,val_output_index,val_pos_index,val_dep_index)\n",
        "def decode_output(output_list):\n",
        "    ix_to_tag = {v:k for k,v in tag_to_ix.items()}\n",
        "    return [ix_to_tag[output] for output in output_list]\n",
        "y_true_decode = decode_output(y_true)\n",
        "y_pred_decode = decode_output(y_pred)\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true_decode,y_pred_decode,digits=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktvvsCojjN71",
        "outputId": "f79f5777-130e-4bd1-ac4f-21b295a41a0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:1, Training loss: 33116.77, train acc: 0.9830, val loss: 2355.28, val acc: 0.9812, time: 651.40s\n",
            "Epoch:2, Training loss: 6398.47, train acc: 0.9935, val loss: 1433.08, val acc: 0.9909, time: 646.57s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9592    0.9746    0.9668      1615\n",
            "           D     0.8065    0.9497    0.8723       338\n",
            "           O     0.9987    0.9908    0.9947     19138\n",
            "           P     0.9929    0.9997    0.9963      3909\n",
            "           S     0.9895    0.9958    0.9926      3301\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.9455    0.9579    0.9517      1450\n",
            "\n",
            "    accuracy                         0.9907     33354\n",
            "   macro avg     0.9560    0.9812    0.9678     33354\n",
            "weighted avg     0.9911    0.9907    0.9908     33354\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.metrics import f1_score\n",
        "sklearn.metrics.f1_score(y_true_decode, y_pred_decode,average='micro')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FunxSFEUkO2-",
        "outputId": "ee9bc4d1-316c-4e8b-fc8d-536ac60c9e7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9907147692882463\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2. Ablation Study 2: Different attention strategy"
      ],
      "metadata": {
        "id": "u_khkRigsaBl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 1"
      ],
      "metadata": {
        "id": "zCJUSv368XdN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculation method is \"scaled dot-product\" and position is in the second layer"
      ],
      "metadata": {
        "id": "WNW49j9p8Zhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from numpy.linalg import norm\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "def argmax(vec):\n",
        "    # return the argmax as a python int\n",
        "    _, idx = torch.max(vec, 1)\n",
        "    return idx.item()\n",
        "\n",
        "\n",
        "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
        "def log_sum_exp(vec):\n",
        "    max_score = vec[0, argmax(vec)]\n",
        "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "    return max_score + \\\n",
        "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
        "\n",
        "class BiLSTM_CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim, wv_embedding_dim, method):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "        self.wv_embedding_dim = wv_embedding_dim\n",
        "        self.method = method\n",
        "        self.word_embeds = nn.Embedding(vocab_size, wv_embedding_dim)\n",
        "        self.word_embeds.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "        \n",
        "        self.pos_embeds = nn.Embedding(pos_embedding.shape[0], pos_embedding.shape[0])\n",
        "        self.pos_embeds.weight.data.copy_(torch.from_numpy(pos_embedding))\n",
        "        \n",
        "        self.dep_embeds = nn.Embedding(dep_embedding.shape[0], dep_embedding.shape[0])\n",
        "        self.dep_embeds.weight.data.copy_(torch.from_numpy(dep_embedding))\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
        "                            num_layers=2, bidirectional=True,dropout=0.5)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim*2, self.tagset_size)\n",
        "\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.hidden_initial()\n",
        "        self.dropout_lstm=nn.Dropout(p=0.5)\n",
        "        \n",
        "    def hidden_initial(self):\n",
        "        return (torch.randn(4, 1, self.hidden_dim // 2).to(device), ### different layers ÈúÄË¶ÅÊîπ\n",
        "                torch.randn(4, 1, self.hidden_dim // 2).to(device)) ###\n",
        "\n",
        "    def _forward_alg(self, feats):\n",
        "        # Do the forward algorithm to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "        #print('_forward_alg')\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of\n",
        "                # the previous tag\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to\n",
        "                # next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the\n",
        "                # edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the\n",
        "                # scores.\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "    # method = dot\n",
        "    def attention(self,lstm_out,hidden_out, hidden_dim, method):\n",
        "\n",
        "        h_out_T = torch.transpose(hidden_out,1,2)\n",
        "        \n",
        "        h_out_T_1 = h_out_T   \n",
        "        h_out_0 = hidden_out   \n",
        "        for i in range(lstm_out.size()[0]-1):\n",
        "          h_out_T = torch.cat((h_out_T, h_out_T_1), 0)\n",
        "          hidden_out = torch.cat((hidden_out, h_out_0), 0)\n",
        "        W = nn.Linear(hidden_dim,hidden_dim,bias=False)\n",
        "        if method == 'dot':\n",
        "            attention_w = F.softmax(torch.bmm(lstm_out, h_out_T),dim=-1)\n",
        "        elif method == 'scaled-dot':\n",
        "            attention_w = F.softmax(torch.bmm(lstm_out, h_out_T)/np.sqrt(self.hidden_dim),dim=-1)\n",
        "        elif method == 'cosine':\n",
        "            attention_w = F.softmax(torch.bmm(lstm_out,h_out_T)/(torch.norm(lstm_out)*torch.norm(h_out_T)),dim=-1)\n",
        "\n",
        "        elif method == 'general':\n",
        "            attention_w = F.softmax(torch.bmm(lstm_out, W(h_out_T), h_out_T),dim=-1)\n",
        "\n",
        "        attention_output = torch.bmm(attention_w, hidden_out)\n",
        "        concat_output = torch.cat((attention_output, lstm_out), 1)\n",
        "        return concat_output\n",
        "\n",
        "    def lstm_ft_get(self, sentence, pos, dep, method):\n",
        "        self.hidden = self.hidden_initial()\n",
        "        #print('lstm_ft_get')\n",
        "\n",
        "        embeds = torch.cat((self.word_embeds(sentence).view(len(sentence), 1, -1),self.pos_embeds(pos).view(len(pos), 1, -1),self.dep_embeds(dep).view(len(dep), 1, -1)),2)\n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        #print('lstm_out.shape',lstm_out.shape)\n",
        "        hidden_out = torch.cat((self.hidden[0].view(2, 2, 1, -1)[1,0,:,:],self.hidden[0].view(2, 2, 1, -1)[1,1,:,:]),1) ###################‰∏çÂêålayerËøôÈáåÈúÄË¶ÅÊîπ\n",
        "        hidden_out = hidden_out.unsqueeze(0)\n",
        "        #print('hidden_out.shape',hidden_out.shape)\n",
        "        attention_out = self.attention(lstm_out,hidden_out,hidden_dim,method)\n",
        "        attention_out = attention_out.view(-1,self.hidden_dim*2)\n",
        "        #print('attention_out.shape',attention_out.shape)\n",
        "        attention_out = self.dropout_lstm(attention_out)\n",
        "        lstm_fts = self.hidden2tag(attention_out)\n",
        "        #print('lstm_fts.shape',lstm_fts.shape)\n",
        "        return lstm_fts\n",
        "\n",
        "    def sentence_scoring(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1).to(device)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long).to(device), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + \\\n",
        "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def viterbi_decoding(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = []  # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                # We don't include the emission scores here because the max\n",
        "                # does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set\n",
        "            # of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "    def neg_ll(self, sentence, tags, pos, dep, method):\n",
        "        feats = self.lstm_ft_get(sentence, pos, dep, method)\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self.sentence_scoring(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    #def forward(self, sentence, ent, pos, dep, bert):  # dont confuse this with _forward_alg above.\n",
        "    def forward(self, sentence, pos, dep):\n",
        "        #print('forward')\n",
        "        # Get the emission scores from the BiLSTM\n",
        "        lstm_fts = self.lstm_ft_get(sentence, pos, dep, method)\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self.viterbi_decoding(lstm_fts)\n",
        "        return score, tag_seq\n",
        "\n",
        "\n",
        "\n",
        "def accuracy_calculation(model, input_index, output_index, pos_index, dep_index):\n",
        "\n",
        "  c, total = 0, 0\n",
        "\n",
        "  pred_list, truth = [], []\n",
        "\n",
        "  #print('accuracy_calculation')\n",
        "  for i in range(len(input_index)):\n",
        "    sentence_in = torch.tensor(input_index[i], dtype=torch.long).to(device)\n",
        "    pos = torch.tensor(pos_index[i], dtype=torch.long).to(device)\n",
        "    dep = torch.tensor(dep_index[i], dtype=torch.long).to(device)\n",
        "\n",
        "\n",
        "    _, pred = model.forward(sentence_in, pos, dep)\n",
        "    for j in range(len(output_index[i])):\n",
        "      total += 1\n",
        "      \n",
        "      pred_list.append(pred[j])\n",
        "      truth.append(output_index[i][j])\n",
        "\n",
        "      if pred[j] == output_index[i][j]:\n",
        "        c += 1\n",
        "\n",
        "  accuracy = c/total\n",
        "\n",
        "  return truth, pred_list, accuracy\n"
      ],
      "metadata": {
        "id": "cYI-gIQm8UF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "hidden_dim = 250\n",
        "\n",
        "model = BiLSTM_CRF(len(full_word_to_ix), tag_to_ix, embedding_dim, hidden_dim, WV_EMBEDDING_DIM, method='scaled-dot').to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, weight_decay=1e-4)\n",
        "\n",
        "import datetime\n",
        "for epoch in range(2):  \n",
        "    time1 = datetime.datetime.now()\n",
        "    train_loss = 0\n",
        "    method='scaled-dot'\n",
        "    model.train()\n",
        "    for i in range(len(train_input_index)):\n",
        "        #print(i)\n",
        "        tags_index = train_output_index[i]\n",
        "        dep_index = train_dep_index[i]\n",
        "        pos_index = train_pos_index[i]\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is,\n",
        "        # turn them into Tensors of word indices.\n",
        "        sentence_in = torch.tensor(train_input_index[i], dtype=torch.long).to(device)\n",
        "        pos = torch.tensor(pos_index, dtype=torch.long).to(device)\n",
        "        dep = torch.tensor(dep_index, dtype=torch.long).to(device)\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "        # Step 3. Run our forward pass.\n",
        "        loss = model.neg_ll(sentence_in, targets, pos, dep, method='scaled-dot')\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        # calling optimizer.step()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss+=loss.item()\n",
        "\n",
        "    model.eval()\n",
        "     # \n",
        "    _, _, train_acc = accuracy_calculation(model,train_input_index,train_output_index,train_pos_index,train_dep_index)\n",
        "    _, _, val_acc = accuracy_calculation(model,val_input_index,val_output_index,val_pos_index,val_dep_index)\n",
        "\n",
        "    val_loss = 0\n",
        "    for i in range(len(val_input_index)):\n",
        "        tags_index = val_output_index[i]\n",
        "        pos_index = val_pos_index[i]\n",
        "        dep_index = val_dep_index[i]\n",
        "        sentence_in = torch.tensor(val_input_index[i], dtype=torch.long).to(device)\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "        pos = torch.tensor(pos_index, dtype=torch.long).to(device)\n",
        "        dep = torch.tensor(dep_index, dtype=torch.long).to(device)\n",
        "        loss = model.neg_ll(sentence_in, targets, pos, dep, method='scaled-dot')\n",
        "        val_loss+=loss.item()\n",
        "    time2 = datetime.datetime.now()\n",
        "\n",
        "    print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))\n",
        "\n",
        "y_pred, y_true, _ = accuracy_calculation(model,val_input_index,val_output_index,val_pos_index,val_dep_index)\n",
        "def decode_output(output_list):\n",
        "    ix_to_tag = {v:k for k,v in tag_to_ix.items()}\n",
        "    return [ix_to_tag[output] for output in output_list]\n",
        "y_true_decode = decode_output(y_true)\n",
        "y_pred_decode = decode_output(y_pred)\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true_decode,y_pred_decode,digits=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f71b88ce-f42a-4c23-f1ae-1b7c06c952bc",
        "id": "ipX_-P7I8UGD"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:1, Training loss: 35176.46, train acc: 0.9870, val loss: 2071.47, val acc: 0.9855, time: 638.67s\n",
            "Epoch:2, Training loss: 5214.27, train acc: 0.9962, val loss: 1309.13, val acc: 0.9926, time: 669.32s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9714    0.9708    0.9711      1642\n",
            "           D     0.8719    0.9666    0.9168       359\n",
            "           O     0.9983    0.9928    0.9955     19089\n",
            "           P     0.9982    0.9990    0.9986      3933\n",
            "           S     0.9868    0.9957    0.9912      3292\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.9551    0.9770    0.9659      1436\n",
            "\n",
            "    accuracy                         0.9926     33354\n",
            "   macro avg     0.9688    0.9860    0.9770     33354\n",
            "weighted avg     0.9928    0.9926    0.9926     33354\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.metrics import f1_score\n",
        "sklearn.metrics.f1_score(y_true_decode, y_pred_decode,average='micro')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02294dd3-0226-4a33-c93e-9a579981ff2c",
        "id": "qBlgPnA28UGE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9925346285303112"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 2"
      ],
      "metadata": {
        "id": "_WulDH_T8y-b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculation method is \"dot-product\" and position is in the second layer\n"
      ],
      "metadata": {
        "id": "c5EdQ5IH89Zt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "hidden_dim = 250\n",
        "\n",
        "model = BiLSTM_CRF(len(full_word_to_ix), tag_to_ix, embedding_dim, hidden_dim, WV_EMBEDDING_DIM, method='dot').to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, weight_decay=1e-4)\n",
        "\n",
        "import datetime\n",
        "for epoch in range(2):  \n",
        "    time1 = datetime.datetime.now()\n",
        "    train_loss = 0\n",
        "    method='dot'\n",
        "    model.train()\n",
        "    for i in range(len(train_input_index)):\n",
        "        #print(i)\n",
        "        tags_index = train_output_index[i]\n",
        "        dep_index = train_dep_index[i]\n",
        "        pos_index = train_pos_index[i]\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is,\n",
        "        # turn them into Tensors of word indices.\n",
        "        sentence_in = torch.tensor(train_input_index[i], dtype=torch.long).to(device)\n",
        "        pos = torch.tensor(pos_index, dtype=torch.long).to(device)\n",
        "        dep = torch.tensor(dep_index, dtype=torch.long).to(device)\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "        # Step 3. Run our forward pass.\n",
        "        loss = model.neg_ll(sentence_in, targets, pos, dep, method='dot')\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        # calling optimizer.step()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss+=loss.item()\n",
        "\n",
        "    model.eval()\n",
        "     # \n",
        "    _, _, train_acc = accuracy_calculation(model,train_input_index,train_output_index,train_pos_index,train_dep_index)\n",
        "    _, _, val_acc = accuracy_calculation(model,val_input_index,val_output_index,val_pos_index,val_dep_index)\n",
        "\n",
        "    val_loss = 0\n",
        "    for i in range(len(val_input_index)):\n",
        "        tags_index = val_output_index[i]\n",
        "        pos_index = val_pos_index[i]\n",
        "        dep_index = val_dep_index[i]\n",
        "        sentence_in = torch.tensor(val_input_index[i], dtype=torch.long).to(device)\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "        pos = torch.tensor(pos_index, dtype=torch.long).to(device)\n",
        "        dep = torch.tensor(dep_index, dtype=torch.long).to(device)\n",
        "        loss = model.neg_ll(sentence_in, targets, pos, dep, method='dot')\n",
        "        val_loss+=loss.item()\n",
        "    time2 = datetime.datetime.now()\n",
        "\n",
        "    print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))\n",
        "\n",
        "y_pred, y_true, _ = accuracy_calculation(model,val_input_index,val_output_index,val_pos_index,val_dep_index)\n",
        "def decode_output(output_list):\n",
        "    ix_to_tag = {v:k for k,v in tag_to_ix.items()}\n",
        "    return [ix_to_tag[output] for output in output_list]\n",
        "y_true_decode = decode_output(y_true)\n",
        "y_pred_decode = decode_output(y_pred)\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true_decode,y_pred_decode,digits=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lxzk3YA8_Cj",
        "outputId": "9b0de4c0-9385-4e24-ca79-d8a0f7703b62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:1, Training loss: 34068.97, train acc: 0.9849, val loss: 2308.79, val acc: 0.9830, time: 642.52s\n",
            "Epoch:2, Training loss: 5765.05, train acc: 0.9944, val loss: 1524.71, val acc: 0.9918, time: 662.61s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9720    0.9667    0.9693      1650\n",
            "           D     0.8216    0.9970    0.9008       328\n",
            "           O     0.9995    0.9912    0.9954     19144\n",
            "           P     0.9967    0.9997    0.9982      3924\n",
            "           S     0.9837    0.9973    0.9905      3277\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.9517    0.9790    0.9651      1428\n",
            "\n",
            "    accuracy                         0.9921     33354\n",
            "   macro avg     0.9607    0.9901    0.9742     33354\n",
            "weighted avg     0.9925    0.9921    0.9922     33354\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.metrics import f1_score\n",
        "sklearn.metrics.f1_score(y_true_decode, y_pred_decode,average='micro')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLjuMFQG9Rb3",
        "outputId": "52d8b0d9-0bc7-44f5-8304-ba98cc6f1a1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9920849073574384"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 3"
      ],
      "metadata": {
        "id": "fquW61np9Aty"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculation method is \"cosine\" and position is in the second layer"
      ],
      "metadata": {
        "id": "sj8p0toU9Enq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "hidden_dim = 250\n",
        "\n",
        "model = BiLSTM_CRF(len(full_word_to_ix), tag_to_ix, embedding_dim, hidden_dim, WV_EMBEDDING_DIM, method='cosine').to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, weight_decay=1e-4)\n",
        "\n",
        "import datetime\n",
        "for epoch in range(2):  \n",
        "    time1 = datetime.datetime.now()\n",
        "    train_loss = 0\n",
        "    method='cosine'\n",
        "    model.train()\n",
        "    for i in range(len(train_input_index)):\n",
        "        #print(i)\n",
        "        tags_index = train_output_index[i]\n",
        "        dep_index = train_dep_index[i]\n",
        "        pos_index = train_pos_index[i]\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is,\n",
        "        # turn them into Tensors of word indices.\n",
        "        sentence_in = torch.tensor(train_input_index[i], dtype=torch.long).to(device)\n",
        "        pos = torch.tensor(pos_index, dtype=torch.long).to(device)\n",
        "        dep = torch.tensor(dep_index, dtype=torch.long).to(device)\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "        # Step 3. Run our forward pass.\n",
        "        loss = model.neg_ll(sentence_in, targets, pos, dep, method='cosine')\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        # calling optimizer.step()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss+=loss.item()\n",
        "\n",
        "    model.eval()\n",
        "     # \n",
        "    _, _, train_acc = accuracy_calculation(model,train_input_index,train_output_index,train_pos_index,train_dep_index)\n",
        "    _, _, val_acc = accuracy_calculation(model,val_input_index,val_output_index,val_pos_index,val_dep_index)\n",
        "\n",
        "    val_loss = 0\n",
        "    for i in range(len(val_input_index)):\n",
        "        tags_index = val_output_index[i]\n",
        "        pos_index = val_pos_index[i]\n",
        "        dep_index = val_dep_index[i]\n",
        "        sentence_in = torch.tensor(val_input_index[i], dtype=torch.long).to(device)\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "        pos = torch.tensor(pos_index, dtype=torch.long).to(device)\n",
        "        dep = torch.tensor(dep_index, dtype=torch.long).to(device)\n",
        "        loss = model.neg_ll(sentence_in, targets, pos, dep, method='cosine')\n",
        "        val_loss+=loss.item()\n",
        "    time2 = datetime.datetime.now()\n",
        "\n",
        "    print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))\n",
        "\n",
        "y_pred, y_true, _ = accuracy_calculation(model,val_input_index,val_output_index,val_pos_index,val_dep_index)\n",
        "def decode_output(output_list):\n",
        "    ix_to_tag = {v:k for k,v in tag_to_ix.items()}\n",
        "    return [ix_to_tag[output] for output in output_list]\n",
        "y_true_decode = decode_output(y_true)\n",
        "y_pred_decode = decode_output(y_pred)\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true_decode,y_pred_decode,digits=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUypz0mO9nNl",
        "outputId": "d29d181d-13da-43ba-d191-8d8d44ed9fc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:1, Training loss: 31311.03, train acc: 0.9830, val loss: 2355.21, val acc: 0.9817, time: 659.33s\n",
            "Epoch:2, Training loss: 6202.13, train acc: 0.9936, val loss: 1506.62, val acc: 0.9906, time: 646.72s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9537    0.9684    0.9610      1616\n",
            "           D     0.8492    0.9854    0.9123       343\n",
            "           O     0.9989    0.9915    0.9952     19127\n",
            "           P     0.9959    0.9995    0.9977      3922\n",
            "           S     0.9786    0.9979    0.9881      3258\n",
            "        SEPA     1.0000    0.9994    0.9997      3605\n",
            "           T     0.9483    0.9393    0.9438      1483\n",
            "\n",
            "    accuracy                         0.9904     33354\n",
            "   macro avg     0.9607    0.9831    0.9711     33354\n",
            "weighted avg     0.9907    0.9904    0.9905     33354\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.metrics import f1_score\n",
        "sklearn.metrics.f1_score(y_true_decode, y_pred_decode,average='micro')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYKuSpAV9owl",
        "outputId": "5b6a6fe0-2fd5-4db0-a205-fae7101fda10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9904359297235714"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 4"
      ],
      "metadata": {
        "id": "_GWRqqR3Ixdf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculation method is \"scaled dot-product\" and position is in the first layer"
      ],
      "metadata": {
        "id": "lMpmCu7PIpq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from numpy.linalg import norm\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "def argmax(vec):\n",
        "    # return the argmax as a python int\n",
        "    _, idx = torch.max(vec, 1)\n",
        "    return idx.item()\n",
        "\n",
        "\n",
        "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
        "def log_sum_exp(vec):\n",
        "    max_score = vec[0, argmax(vec)]\n",
        "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "    return max_score + \\\n",
        "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
        "\n",
        "class BiLSTM_CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim, wv_embedding_dim, method):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "        self.wv_embedding_dim = wv_embedding_dim\n",
        "        self.method = method\n",
        "        self.word_embeds = nn.Embedding(vocab_size, wv_embedding_dim)\n",
        "        self.word_embeds.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "        \n",
        "        self.pos_embeds = nn.Embedding(pos_embedding.shape[0], pos_embedding.shape[0])\n",
        "        self.pos_embeds.weight.data.copy_(torch.from_numpy(pos_embedding))\n",
        "        \n",
        "        self.dep_embeds = nn.Embedding(dep_embedding.shape[0], dep_embedding.shape[0])\n",
        "        self.dep_embeds.weight.data.copy_(torch.from_numpy(dep_embedding))\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
        "                            num_layers=2, bidirectional=True,dropout=0.5)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim*2, self.tagset_size)\n",
        "\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.hidden_initial()\n",
        "        self.dropout_lstm=nn.Dropout(p=0.5)\n",
        "        \n",
        "    def hidden_initial(self):\n",
        "        return (torch.randn(4, 1, self.hidden_dim // 2).to(device), ### different layers ÈúÄË¶ÅÊîπ\n",
        "                torch.randn(4, 1, self.hidden_dim // 2).to(device)) ###\n",
        "\n",
        "    def _forward_alg(self, feats):\n",
        "        # Do the forward algorithm to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "        #print('_forward_alg')\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of\n",
        "                # the previous tag\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to\n",
        "                # next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the\n",
        "                # edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the\n",
        "                # scores.\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "    # method = dot\n",
        "    def attention(self,lstm_out,hidden_out, hidden_dim, method):\n",
        "        #print('attention')\n",
        "        #h_out_T = torch.transpose(hidden_out,0,1)\n",
        "        h_out_T = torch.transpose(hidden_out,1,2)\n",
        "        \n",
        "        h_out_T_1 = h_out_T   \n",
        "        h_out_0 = hidden_out\n",
        "        for i in range(lstm_out.size()[0]-1):\n",
        "\n",
        "          h_out_T = torch.cat((h_out_T, h_out_T_1), 0)\n",
        "          hidden_out = torch.cat((hidden_out, h_out_0), 0)\n",
        "        W = nn.Linear(hidden_dim,hidden_dim,bias=False)\n",
        "\n",
        "        #\n",
        "        #print('lstm_out',lstm_out.size())\n",
        "        #print('h_out_T',h_out_T.size())\n",
        "        #print('hidden_out',hidden_out.size())\n",
        "\n",
        "        if method == 'dot':\n",
        "            attention_w = F.softmax(torch.bmm(lstm_out, h_out_T),dim=-1)\n",
        "        elif method == 'scaled-dot':\n",
        "            attention_w = F.softmax(torch.bmm(lstm_out, h_out_T)/np.sqrt(self.hidden_dim),dim=-1)\n",
        "        elif method == 'cosine':\n",
        "            attention_w = F.softmax(torch.bmm(lstm_out,h_out_T)/(torch.norm(lstm_out)*torch.norm(h_out_T)),dim=-1)\n",
        "\n",
        "        #elif method == 'general':\n",
        "        #    attention_w = F.softmax(torch.bmm(lstm_out, W(h_out_T), h_out_T),dim=-1)\n",
        "\n",
        "        attention_output = torch.bmm(attention_w, hidden_out)\n",
        "        concat_output = torch.cat((attention_output, lstm_out), 1)  \n",
        "        return concat_output\n",
        "\n",
        "    def lstm_ft_get(self, sentence, pos, dep, method):\n",
        "        self.hidden = self.hidden_initial()\n",
        "        #print('lstm_ft_get')\n",
        "\n",
        "        embeds = torch.cat((self.word_embeds(sentence).view(len(sentence), 1, -1),self.pos_embeds(pos).view(len(pos), 1, -1),self.dep_embeds(dep).view(len(dep), 1, -1)),2)\n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "\n",
        "\n",
        "        #print('lstm_out.shape',lstm_out.shape)\n",
        "        hidden_out = torch.cat((self.hidden[0].view(2, 2, 1, -1)[0,0,:,:],self.hidden[0].view(2, 2, 1, -1)[0,1,:,:]),1) ###################‰∏çÂêålayerËøôÈáåÈúÄË¶ÅÊîπ\n",
        "        hidden_out = hidden_out.unsqueeze(0)\n",
        "        #print('hidden_out.shape',hidden_out.shape)\n",
        "        attention_out = self.attention(lstm_out,hidden_out,hidden_dim,method)\n",
        "        attention_out = attention_out.view(-1,self.hidden_dim*2)\n",
        "\n",
        "        #print('attention_out.shape',attention_out.shape)\n",
        "        attention_out = self.dropout_lstm(attention_out)\n",
        "        lstm_fts = self.hidden2tag(attention_out)\n",
        "        #print('lstm_fts.shape',lstm_fts.shape)\n",
        "        return lstm_fts\n",
        "\n",
        "    def sentence_scoring(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1).to(device)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long).to(device), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + \\\n",
        "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def viterbi_decoding(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = []  # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                # We don't include the emission scores here because the max\n",
        "                # does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set\n",
        "            # of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "    def neg_ll(self, sentence, tags, pos, dep, method):\n",
        "        feats = self.lstm_ft_get(sentence, pos, dep, method)\n",
        "        #print('neg_ll')\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self.sentence_scoring(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    #def forward(self, sentence, ent, pos, dep, bert):  # dont confuse this with _forward_alg above.\n",
        "    def forward(self, sentence, pos, dep):\n",
        "        #print('forward')\n",
        "        # Get the emission scores from the BiLSTM\n",
        "\n",
        "        lstm_fts = self.lstm_ft_get(sentence, pos, dep, method)\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self.viterbi_decoding(lstm_fts)\n",
        "        return score, tag_seq\n",
        "\n",
        "\n",
        "\n",
        "def accuracy_calculation(model, input_index, output_index, pos_index, dep_index):\n",
        "\n",
        "  c, total = 0, 0\n",
        "\n",
        "  pred_list, truth = [], []\n",
        "\n",
        "  #print('accuracy_calculation')\n",
        "for i in range(len(input_index)):\n",
        "    sentence_in = torch.tensor(input_index[i], dtype=torch.long).to(device)\n",
        "    \n",
        "    pos = torch.tensor(pos_index[i], dtype=torch.long).to(device)\n",
        "    dep = torch.tensor(dep_index[i], dtype=torch.long).to(device)\n",
        "\n",
        "\n",
        "\n",
        "    _, pred = model.forward(sentence_in, pos, dep)\n",
        "    for j in range(len(output_index[i])):\n",
        "      total += 1\n",
        "      \n",
        "      pred_list.append(pred[j])\n",
        "      truth.append(output_index[i][j])\n",
        "\n",
        "      if pred[j] == output_index[i][j]:\n",
        "        c += 1\n",
        "\n",
        "  accuracy = c/total\n",
        "\n",
        "  return truth, pred_list, accuracy\n"
      ],
      "metadata": {
        "id": "Nev0caG6PRgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_calculation(model, input_index, output_index, pos_index, dep_index):\n",
        "\n",
        "  c, total = 0, 0\n",
        "\n",
        "  pred_list, truth = [], []\n",
        "\n",
        "  #print('accuracy_calculation')\n",
        "  for i in range(len(input_index)):\n",
        "    sentence_in = torch.tensor(input_index[i], dtype=torch.long).to(device)\n",
        "    pos = torch.tensor(pos_index[i], dtype=torch.long).to(device)\n",
        "    dep = torch.tensor(dep_index[i], dtype=torch.long).to(device)\n",
        "\n",
        "\n",
        "\n",
        "    _, pred = model.forward(sentence_in, pos, dep)\n",
        "    for j in range(len(output_index[i])):\n",
        "      total += 1\n",
        "      \n",
        "      pred_list.append(pred[j])\n",
        "      truth.append(output_index[i][j])\n",
        "\n",
        "      if pred[j] == output_index[i][j]:\n",
        "        c += 1\n",
        "\n",
        "  accuracy = c/total\n",
        "\n",
        "  return truth, pred_list, accuracy"
      ],
      "metadata": {
        "id": "OOuhjMy_PUa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "hidden_dim = 250\n",
        "\n",
        "model = BiLSTM_CRF(len(full_word_to_ix), tag_to_ix, embedding_dim, hidden_dim, WV_EMBEDDING_DIM, method='scaled-dot').to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, weight_decay=1e-4)\n",
        "\n",
        "import datetime\n",
        "for epoch in range(2):  \n",
        "    time1 = datetime.datetime.now()\n",
        "    train_loss = 0\n",
        "    method='scaled-dot'\n",
        "    model.train()\n",
        "    for i in range(len(train_input_index)):\n",
        "        tags_index = train_output_index[i]\n",
        "        dep_index = train_dep_index[i]\n",
        "        pos_index = train_pos_index[i]\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is,\n",
        "        # turn them into Tensors of word indices.\n",
        "        sentence_in = torch.tensor(train_input_index[i], dtype=torch.long).to(device)\n",
        "        pos = torch.tensor(pos_index, dtype=torch.long).to(device)\n",
        "        dep = torch.tensor(dep_index, dtype=torch.long).to(device)\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "        # Step 3. Run our forward pass.\n",
        "        loss = model.neg_ll(sentence_in, targets, pos, dep, method='scaled-dot')\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        # calling optimizer.step()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss+=loss.item()\n",
        "    model.eval()\n",
        "     # \n",
        "    _, _, train_acc = accuracy_calculation(model,train_input_index,train_output_index,train_pos_index,train_dep_index)\n",
        "    _, _, val_acc = accuracy_calculation(model,val_input_index,val_output_index,val_pos_index,val_dep_index)\n",
        "\n",
        "    val_loss = 0\n",
        "    for i in range(len(val_input_index)):\n",
        "        tags_index = val_output_index[i]\n",
        "        pos_index = val_pos_index[i]\n",
        "        dep_index = val_dep_index[i]\n",
        "        sentence_in = torch.tensor(val_input_index[i], dtype=torch.long).to(device)\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "        pos = torch.tensor(pos_index, dtype=torch.long).to(device)\n",
        "        dep = torch.tensor(dep_index, dtype=torch.long).to(device)\n",
        "        loss = model.neg_ll(sentence_in, targets, pos, dep, method='scaled-dot')\n",
        "        val_loss+=loss.item()\n",
        "    time2 = datetime.datetime.now()\n",
        "\n",
        "    print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))\n",
        "\n",
        "y_pred, y_true, _ = accuracy_calculation(model,val_input_index,val_output_index,val_pos_index,val_dep_index)\n",
        "def decode_output(output_list):\n",
        "    ix_to_tag = {v:k for k,v in tag_to_ix.items()}\n",
        "    return [ix_to_tag[output] for output in output_list]\n",
        "y_true_decode = decode_output(y_true)\n",
        "y_pred_decode = decode_output(y_pred)\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true_decode,y_pred_decode,digits=4))\n"
      ],
      "metadata": {
        "id": "EnQ8dzEePXqO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a904daac-244c-4f21-e3a8-347b987fc043"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:1, Training loss: 38487.36, train acc: 0.9859, val loss: 2432.55, val acc: 0.9845, time: 754.53s\n",
            "Epoch:2, Training loss: 6977.49, train acc: 0.9945, val loss: 1671.03, val acc: 0.9921, time: 736.60s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9567    0.9764    0.9665      1608\n",
            "           D     0.8844    0.9239    0.9037       381\n",
            "           O     0.9992    0.9920    0.9956     19123\n",
            "           P     0.9975    0.9997    0.9986      3927\n",
            "           S     0.9849    0.9976    0.9912      3280\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.9455    0.9700    0.9576      1432\n",
            "\n",
            "    accuracy                         0.9918     33354\n",
            "   macro avg     0.9669    0.9799    0.9733     33354\n",
            "weighted avg     0.9920    0.9918    0.9919     33354\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.metrics import f1_score\n",
        "sklearn.metrics.f1_score(y_true_decode, y_pred_decode,average='micro')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmjBRfJwX83q",
        "outputId": "acc127ca-f124-4c0e-e664-788ae6301981"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9918450560652395"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 5"
      ],
      "metadata": {
        "id": "RhRhaVbKU191"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculation method is \"scaled dot-product\" and position is in the both 2 attention layers (averaged)"
      ],
      "metadata": {
        "id": "NXo37j9cVDNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from numpy.linalg import norm\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "def argmax(vec):\n",
        "    # return the argmax as a python int\n",
        "    _, idx = torch.max(vec, 1)\n",
        "    return idx.item()\n",
        "\n",
        "\n",
        "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
        "def log_sum_exp(vec):\n",
        "    max_score = vec[0, argmax(vec)]\n",
        "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "    return max_score + \\\n",
        "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
        "\n",
        "class BiLSTM_CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim, wv_embedding_dim, method):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "        self.wv_embedding_dim = wv_embedding_dim\n",
        "        self.method = method\n",
        "        self.word_embeds = nn.Embedding(vocab_size, wv_embedding_dim)\n",
        "        self.word_embeds.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "        \n",
        "        self.pos_embeds = nn.Embedding(pos_embedding.shape[0], pos_embedding.shape[0])\n",
        "        self.pos_embeds.weight.data.copy_(torch.from_numpy(pos_embedding))\n",
        "        \n",
        "        self.dep_embeds = nn.Embedding(dep_embedding.shape[0], dep_embedding.shape[0])\n",
        "        self.dep_embeds.weight.data.copy_(torch.from_numpy(dep_embedding))\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
        "                            num_layers=2, bidirectional=True,dropout=0.5)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim*2, self.tagset_size)\n",
        "\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.hidden_initial()\n",
        "        self.dropout_lstm=nn.Dropout(p=0.5)\n",
        "        \n",
        "    def hidden_initial(self):\n",
        "        return (torch.randn(4, 1, self.hidden_dim // 2).to(device), ### different layers ÈúÄË¶ÅÊîπ\n",
        "                torch.randn(4, 1, self.hidden_dim // 2).to(device)) ###\n",
        "\n",
        "    def _forward_alg(self, feats):\n",
        "        # Do the forward algorithm to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "        #print('_forward_alg')\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of\n",
        "                # the previous tag\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to\n",
        "                # next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the\n",
        "                # edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the\n",
        "                # scores.\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "    # method = dot\n",
        "    def attention(self,lstm_out,hidden_out, hidden_dim, method):\n",
        "        #print('attention')\n",
        "        #h_out_T = torch.transpose(hidden_out,0,1)\n",
        "        h_out_T = torch.transpose(hidden_out,1,2)\n",
        "        \n",
        "        h_out_T_1 = h_out_T   \n",
        "        h_out_0 = hidden_out\n",
        "        for i in range(lstm_out.size()[0]-1):\n",
        "\n",
        "          h_out_T = torch.cat((h_out_T, h_out_T_1), 0)\n",
        "          hidden_out = torch.cat((hidden_out, h_out_0), 0)\n",
        "        W = nn.Linear(hidden_dim,hidden_dim,bias=False)\n",
        "\n",
        "        if method == 'dot':\n",
        "            attention_w = F.softmax(torch.bmm(lstm_out, h_out_T),dim=-1)\n",
        "        elif method == 'scaled-dot':\n",
        "            attention_w = F.softmax(torch.bmm(lstm_out, h_out_T)/np.sqrt(self.hidden_dim),dim=-1)\n",
        "        elif method == 'cosine':\n",
        "            attention_w = F.softmax(torch.bmm(lstm_out,h_out_T)/(torch.norm(lstm_out)*torch.norm(h_out_T)),dim=-1)\n",
        "\n",
        "\n",
        "        attention_output = torch.bmm(attention_w, hidden_out)\n",
        "        concat_output = torch.cat((attention_output, lstm_out), 1)  \n",
        "        return concat_output\n",
        "\n",
        "    def lstm_ft_get(self, sentence, pos, dep, method):\n",
        "        self.hidden = self.hidden_initial()\n",
        "\n",
        "        embeds = torch.cat((self.word_embeds(sentence).view(len(sentence), 1, -1),self.pos_embeds(pos).view(len(pos), 1, -1),self.dep_embeds(dep).view(len(dep), 1, -1)),2)\n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "\n",
        "        #print('lstm_out.shape',lstm_out.shape)\n",
        "        hidden_out = torch.cat((self.hidden[0].view(2, 2, 1, -1)[0,0,:,:],self.hidden[0].view(2, 2, 1, -1)[0,1,:,:]),1) ###################‰∏çÂêålayerËøôÈáåÈúÄË¶ÅÊîπ\n",
        "        hidden_out = hidden_out.unsqueeze(0)\n",
        "        #print('hidden_out.shape',hidden_out.shape)\n",
        "        attention_out_1 = self.attention(lstm_out,hidden_out,hidden_dim,method)\n",
        "        attention_out_1 = attention_out_1.view(-1,self.hidden_dim*2)\n",
        "\n",
        "        #print('lstm_out.shape',lstm_out.shape)\n",
        "        hidden_out = torch.cat((self.hidden[0].view(2, 2, 1, -1)[1,0,:,:],self.hidden[0].view(2, 2, 1, -1)[1,1,:,:]),1) ###################‰∏çÂêålayerËøôÈáåÈúÄË¶ÅÊîπ\n",
        "        hidden_out = hidden_out.unsqueeze(0)\n",
        "        #print('hidden_out.shape',hidden_out.shape)\n",
        "        attention_out_2 = self.attention(lstm_out,hidden_out,hidden_dim,method)\n",
        "        attention_out_2 = attention_out_2.view(-1,self.hidden_dim*2)\n",
        "\n",
        "        w = 0.5\n",
        "        attention_out = w*attention_out_2 + (1-w)*attention_out_1\n",
        "\n",
        "        #print('attention_out.shape',attention_out.shape)\n",
        "        attention_out = self.dropout_lstm(attention_out)\n",
        "        lstm_fts = self.hidden2tag(attention_out)\n",
        "        #print('lstm_fts.shape',lstm_fts.shape)\n",
        "        return lstm_fts\n",
        "\n",
        "    def sentence_scoring(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1).to(device)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long).to(device), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + \\\n",
        "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def viterbi_decoding(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = []  # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                # We don't include the emission scores here because the max\n",
        "                # does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set\n",
        "            # of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "    def neg_ll(self, sentence, tags, pos, dep, method):\n",
        "        feats = self.lstm_ft_get(sentence, pos, dep, method)\n",
        "        #print('neg_ll')\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self.sentence_scoring(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    #def forward(self, sentence, ent, pos, dep, bert):  # dont confuse this with _forward_alg above.\n",
        "    def forward(self, sentence, pos, dep):\n",
        "        #print('forward')\n",
        "        # Get the emission scores from the BiLSTM\n",
        "\n",
        "        lstm_fts = self.lstm_ft_get(sentence, pos, dep, method)\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self.viterbi_decoding(lstm_fts)\n",
        "        return score, tag_seq\n",
        "\n",
        "\n",
        "\n",
        "def accuracy_calculation(model, input_index, output_index, pos_index, dep_index):\n",
        "\n",
        "  c, total = 0, 0\n",
        "\n",
        "  pred_list, truth = [], []\n",
        "\n",
        "  #print('accuracy_calculation')\n",
        "  for i in range(len(input_index)):\n",
        "    sentence_in = torch.tensor(input_index[i], dtype=torch.long).to(device)\n",
        "    pos = torch.tensor(pos_index[i], dtype=torch.long).to(device)\n",
        "    dep = torch.tensor(dep_index[i], dtype=torch.long).to(device)\n",
        "\n",
        "\n",
        "    _, pred = model.forward(sentence_in, pos, dep)\n",
        "    for j in range(len(output_index[i])):\n",
        "      total += 1\n",
        "      \n",
        "      pred_list.append(pred[j])\n",
        "      truth.append(output_index[i][j])\n",
        "\n",
        "      if pred[j] == output_index[i][j]:\n",
        "        c += 1\n",
        "\n",
        "  accuracy = c/total\n",
        "\n",
        "  return truth, pred_list, accuracy\n"
      ],
      "metadata": {
        "id": "mkgCrFJz_dqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_calculation(model, input_index, output_index, pos_index, dep_index):\n",
        "\n",
        "  c, total = 0, 0\n",
        "\n",
        "  pred_list, truth = [], []\n",
        "\n",
        "  #print('accuracy_calculation')\n",
        "  for i in range(len(input_index)):\n",
        "    sentence_in = torch.tensor(input_index[i], dtype=torch.long).to(device)\n",
        "    pos = torch.tensor(pos_index[i], dtype=torch.long).to(device)\n",
        "    dep = torch.tensor(dep_index[i], dtype=torch.long).to(device)\n",
        "\n",
        "    _, pred = model.forward(sentence_in, pos, dep)\n",
        "    for j in range(len(output_index[i])):\n",
        "      total += 1\n",
        "      \n",
        "      pred_list.append(pred[j])\n",
        "      truth.append(output_index[i][j])\n",
        "\n",
        "      if pred[j] == output_index[i][j]:\n",
        "        c += 1\n",
        "\n",
        "  accuracy = c/total\n",
        "\n",
        "  return truth, pred_list, accuracy"
      ],
      "metadata": {
        "id": "NjQpGrJo_gOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "hidden_dim = 250\n",
        "\n",
        "model = BiLSTM_CRF(len(full_word_to_ix), tag_to_ix, embedding_dim, hidden_dim, WV_EMBEDDING_DIM, method='scaled-dot').to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, weight_decay=1e-4)\n",
        "\n",
        "import datetime\n",
        "for epoch in range(2):  \n",
        "    time1 = datetime.datetime.now()\n",
        "    train_loss = 0\n",
        "    method='scaled-dot'\n",
        "    model.train()\n",
        "    for i in range(len(train_input_index)):\n",
        "        tags_index = train_output_index[i]\n",
        "        dep_index = train_dep_index[i]\n",
        "        pos_index = train_pos_index[i]\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is,\n",
        "        # turn them into Tensors of word indices.\n",
        "        sentence_in = torch.tensor(train_input_index[i], dtype=torch.long).to(device)\n",
        "        pos = torch.tensor(pos_index, dtype=torch.long).to(device)\n",
        "        dep = torch.tensor(dep_index, dtype=torch.long).to(device)\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "        # Step 3. Run our forward pass.\n",
        "        loss = model.neg_ll(sentence_in, targets, pos, dep, method='scaled-dot')\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        # calling optimizer.step()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss+=loss.item()\n",
        "    model.eval()\n",
        "     # \n",
        "    _, _, train_acc = accuracy_calculation(model,train_input_index,train_output_index,train_pos_index,train_dep_index)\n",
        "    _, _, val_acc = accuracy_calculation(model,val_input_index,val_output_index,val_pos_index,val_dep_index)\n",
        "\n",
        "    val_loss = 0\n",
        "    for i in range(len(val_input_index)):\n",
        "        tags_index = val_output_index[i]\n",
        "        pos_index = val_pos_index[i]\n",
        "        dep_index = val_dep_index[i]\n",
        "        sentence_in = torch.tensor(val_input_index[i], dtype=torch.long).to(device)\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "        pos = torch.tensor(pos_index, dtype=torch.long).to(device)\n",
        "        dep = torch.tensor(dep_index, dtype=torch.long).to(device)\n",
        "        loss = model.neg_ll(sentence_in, targets, pos, dep, method='scaled-dot')\n",
        "        val_loss+=loss.item()\n",
        "    time2 = datetime.datetime.now()\n",
        "\n",
        "    print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))\n",
        "\n",
        "y_pred, y_true, _ = accuracy_calculation(model,val_input_index,val_output_index,val_pos_index,val_dep_index)\n",
        "def decode_output(output_list):\n",
        "    ix_to_tag = {v:k for k,v in tag_to_ix.items()}\n",
        "    return [ix_to_tag[output] for output in output_list]\n",
        "y_true_decode = decode_output(y_true)\n",
        "y_pred_decode = decode_output(y_pred)\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true_decode,y_pred_decode,digits=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9atMwOe-0Gd",
        "outputId": "58752fb0-97de-4567-89af-c0bb4e75a88b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:1, Training loss: 30269.71, train acc: 0.9852, val loss: 2316.17, val acc: 0.9832, time: 802.12s\n",
            "Epoch:2, Training loss: 5596.48, train acc: 0.9937, val loss: 1517.98, val acc: 0.9903, time: 778.51s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9580    0.9638    0.9609      1631\n",
            "           D     0.7312    0.9151    0.8128       318\n",
            "           O     0.9991    0.9916    0.9953     19127\n",
            "           P     0.9964    0.9990    0.9977      3926\n",
            "           S     0.9862    0.9967    0.9914      3287\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.9489    0.9535    0.9512      1462\n",
            "\n",
            "    accuracy                         0.9901     33354\n",
            "   macro avg     0.9457    0.9742    0.9585     33354\n",
            "weighted avg     0.9908    0.9901    0.9904     33354\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.metrics import f1_score\n",
        "sklearn.metrics.f1_score(y_true_decode, y_pred_decode,average='micro')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AV-bpnrCX15y",
        "outputId": "1607e6cc-dbd3-4c18-8cd6-23a4d9840335"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9901361156083228"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3. Ablation Study 3: Different stacked layers"
      ],
      "metadata": {
        "id": "ormGbzrVsiNq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 1"
      ],
      "metadata": {
        "id": "6WxpKcqkonHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Numer of layer =1"
      ],
      "metadata": {
        "id": "bL3N_lWvoH58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from numpy.linalg import norm\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "def argmax(vec):\n",
        "    # return the argmax as a python int\n",
        "    _, idx = torch.max(vec, 1)\n",
        "    return idx.item()\n",
        "\n",
        "\n",
        "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
        "def log_sum_exp(vec):\n",
        "    max_score = vec[0, argmax(vec)]\n",
        "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "    return max_score + \\\n",
        "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
        "\n",
        "class BiLSTM_CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim, wv_embedding_dim, method):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "        self.wv_embedding_dim = wv_embedding_dim\n",
        "        self.method = method\n",
        "        self.word_embeds = nn.Embedding(vocab_size, wv_embedding_dim)\n",
        "        self.word_embeds.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "        \n",
        "        self.pos_embeds = nn.Embedding(pos_embedding.shape[0], pos_embedding.shape[0])\n",
        "        self.pos_embeds.weight.data.copy_(torch.from_numpy(pos_embedding))\n",
        "        \n",
        "        self.dep_embeds = nn.Embedding(dep_embedding.shape[0], dep_embedding.shape[0])\n",
        "        self.dep_embeds.weight.data.copy_(torch.from_numpy(dep_embedding))\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
        "                            num_layers=1, bidirectional=True,dropout=0.5)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim*2, self.tagset_size)\n",
        "\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.hidden_initial()\n",
        "        self.dropout_lstm=nn.Dropout(p=0.5)\n",
        "        \n",
        "    def hidden_initial(self):\n",
        "        return (torch.randn(2, 1, self.hidden_dim // 2).to(device), ### different layers ÈúÄË¶ÅÊîπ\n",
        "                torch.randn(2, 1, self.hidden_dim // 2).to(device)) ###\n",
        "\n",
        "    def _forward_alg(self, feats):\n",
        "        # Do the forward algorithm to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "        #print('_forward_alg')\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of\n",
        "                # the previous tag\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to\n",
        "                # next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the\n",
        "                # edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the\n",
        "                # scores.\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "    # method = dot\n",
        "    def attention(self,lstm_out,hidden_out, hidden_dim, method):\n",
        "\n",
        "        h_out_T = torch.transpose(hidden_out,1,2)\n",
        "        \n",
        "        h_out_T_1 = h_out_T   \n",
        "        h_out_0 = hidden_out   \n",
        "        for i in range(lstm_out.size()[0]-1):\n",
        "          h_out_T = torch.cat((h_out_T, h_out_T_1), 0)\n",
        "          hidden_out = torch.cat((hidden_out, h_out_0), 0)\n",
        "        W = nn.Linear(hidden_dim,hidden_dim,bias=False)\n",
        "        if method == 'dot':\n",
        "            attention_w = F.softmax(torch.bmm(lstm_out, h_out_T),dim=-1)\n",
        "        elif method == 'scaled-dot':\n",
        "            attention_w = F.softmax(torch.bmm(lstm_out, h_out_T)/np.sqrt(self.hidden_dim),dim=-1)\n",
        "        elif method == 'cosine':\n",
        "            attention_w = F.softmax(torch.bmm(lstm_out,h_out_T)/(torch.norm(lstm_out)*torch.norm(h_out_T)),dim=-1)\n",
        "\n",
        "        elif method == 'general':\n",
        "            attention_w = F.softmax(torch.bmm(lstm_out, W(h_out_T), h_out_T),dim=-1)\n",
        "\n",
        "        attention_output = torch.bmm(attention_w, hidden_out)\n",
        "        concat_output = torch.cat((attention_output, lstm_out), 1)\n",
        "        return concat_output\n",
        "\n",
        "    def lstm_ft_get(self, sentence, pos, dep, method):\n",
        "        self.hidden = self.hidden_initial()\n",
        "        embeds = torch.cat((self.word_embeds(sentence).view(len(sentence), 1, -1),self.pos_embeds(pos).view(len(pos), 1, -1),self.dep_embeds(dep).view(len(dep), 1, -1)),2)\n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        #print('lstm_out.shape',lstm_out.shape)\n",
        "        hidden_out = torch.cat((self.hidden[0].view(1, 2, 1, -1)[0,0,:,:],self.hidden[0].view(1, 2, 1, -1)[0,1,:,:]),1) ###################‰∏çÂêålayerËøôÈáåÈúÄË¶ÅÊîπ\n",
        "        hidden_out = hidden_out.unsqueeze(0)\n",
        "        #print('hidden_out.shape',hidden_out.shape)\n",
        "        attention_out = self.attention(lstm_out,hidden_out,hidden_dim,method)\n",
        "        attention_out = attention_out.view(-1,self.hidden_dim*2)\n",
        "        #print('attention_out.shape',attention_out.shape)\n",
        "        attention_out = self.dropout_lstm(attention_out)\n",
        "        lstm_fts = self.hidden2tag(attention_out)\n",
        "        #print('lstm_fts.shape',lstm_fts.shape)\n",
        "        return lstm_fts\n",
        "\n",
        "    def sentence_scoring(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1).to(device)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long).to(device), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + \\\n",
        "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def viterbi_decoding(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = []  # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                # We don't include the emission scores here because the max\n",
        "                # does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set\n",
        "            # of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "    def neg_ll(self, sentence, tags, pos, dep, method):\n",
        "        feats = self.lstm_ft_get(sentence, pos, dep, method)\n",
        "        #print('neg_ll')\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self.sentence_scoring(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    #def forward(self, sentence, ent, pos, dep, bert):  # dont confuse this with _forward_alg above.\n",
        "    def forward(self, sentence, pos, dep):\n",
        "        #print('forward')\n",
        "        # Get the emission scores from the BiLSTM\n",
        "\n",
        "        lstm_fts = self.lstm_ft_get(sentence, pos, dep, method)\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self.viterbi_decoding(lstm_fts)\n",
        "        return score, tag_seq\n",
        "\n",
        "def accuracy_calculation(model, input_index, output_index, pos_index, dep_index):\n",
        "\n",
        "  c, total = 0, 0\n",
        "\n",
        "  pred_list, truth = [], []\n",
        "\n",
        "  #print('accuracy_calculation')\n",
        "  for i in range(len(input_index)):\n",
        "    sentence_in = torch.tensor(input_index[i], dtype=torch.long).to(device)\n",
        "    pos = torch.tensor(pos_index[i], dtype=torch.long).to(device)\n",
        "    dep = torch.tensor(dep_index[i], dtype=torch.long).to(device)\n",
        "\n",
        "    _, pred = model.forward(sentence_in, pos, dep)\n",
        "    for j in range(len(output_index[i])):\n",
        "      total += 1\n",
        "      pred_list.append(pred[j])\n",
        "      truth.append(output_index[i][j])\n",
        "      if pred[j] == output_index[i][j]:\n",
        "        c += 1\n",
        "  accuracy = c/total\n",
        "  return truth, pred_list, accuracy\n"
      ],
      "metadata": {
        "id": "Cj62X9VSZKBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "hidden_dim = 250\n",
        "\n",
        "model = BiLSTM_CRF(len(full_word_to_ix), tag_to_ix, embedding_dim, hidden_dim, WV_EMBEDDING_DIM, method='scaled-dot').to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, weight_decay=1e-4)\n",
        "\n",
        "import datetime\n",
        "for epoch in range(2):  \n",
        "    time1 = datetime.datetime.now()\n",
        "    train_loss = 0\n",
        "    method='scaled-dot'\n",
        "    model.train()\n",
        "    for i in range(len(train_input_index)):\n",
        "        #print(i)\n",
        "        tags_index = train_output_index[i]\n",
        "        dep_index = train_dep_index[i]\n",
        "        pos_index = train_pos_index[i]\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is,\n",
        "        # turn them into Tensors of word indices.\n",
        "        sentence_in = torch.tensor(train_input_index[i], dtype=torch.long).to(device)\n",
        "        pos = torch.tensor(pos_index, dtype=torch.long).to(device)\n",
        "        dep = torch.tensor(dep_index, dtype=torch.long).to(device)\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "        # Step 3. Run our forward pass.\n",
        "        loss = model.neg_ll(sentence_in, targets, pos, dep, method='scaled-dot')\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        # calling optimizer.step()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss+=loss.item()\n",
        "\n",
        "    model.eval()\n",
        "     # \n",
        "    _, _, train_acc = accuracy_calculation(model,train_input_index,train_output_index,train_pos_index,train_dep_index)\n",
        "    _, _, val_acc = accuracy_calculation(model,val_input_index,val_output_index,val_pos_index,val_dep_index)\n",
        "\n",
        "    val_loss = 0\n",
        "    for i in range(len(val_input_index)):\n",
        "        tags_index = val_output_index[i]\n",
        "        pos_index = val_pos_index[i]\n",
        "        dep_index = val_dep_index[i]\n",
        "        sentence_in = torch.tensor(val_input_index[i], dtype=torch.long).to(device)\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "        pos = torch.tensor(pos_index, dtype=torch.long).to(device)\n",
        "        dep = torch.tensor(dep_index, dtype=torch.long).to(device)\n",
        "        loss = model.neg_ll(sentence_in, targets, pos, dep, method='scaled-dot')\n",
        "        val_loss+=loss.item()\n",
        "    time2 = datetime.datetime.now()\n",
        "\n",
        "    print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))\n",
        "\n",
        "y_pred, y_true, _ = accuracy_calculation(model,val_input_index,val_output_index,val_pos_index,val_dep_index)\n",
        "def decode_output(output_list):\n",
        "    ix_to_tag = {v:k for k,v in tag_to_ix.items()}\n",
        "    return [ix_to_tag[output] for output in output_list]\n",
        "y_true_decode = decode_output(y_true)\n",
        "y_pred_decode = decode_output(y_pred)\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true_decode,y_pred_decode,digits=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e88ee77-21fb-43b6-a5d2-09336737ab20",
        "id": "eoDIUaV2ZKBH"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:1, Training loss: 24818.84, train acc: 0.9912, val loss: 1663.70, val acc: 0.9886, time: 611.66s\n",
            "Epoch:2, Training loss: 3723.22, train acc: 0.9979, val loss: 1371.56, val acc: 0.9946, time: 619.91s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9707    0.9956    0.9830      1600\n",
            "           D     0.9296    0.9763    0.9524       379\n",
            "           O     0.9997    0.9926    0.9961     19120\n",
            "           P     0.9982    0.9995    0.9989      3931\n",
            "           S     0.9898    0.9994    0.9946      3290\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.9598    0.9853    0.9724      1431\n",
            "\n",
            "    accuracy                         0.9945     33354\n",
            "   macro avg     0.9783    0.9927    0.9853     33354\n",
            "weighted avg     0.9947    0.9945    0.9946     33354\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.metrics import f1_score\n",
        "sklearn.metrics.f1_score(y_true_decode, y_pred_decode,average='micro')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa713f8f-4e35-4053-a5ba-2c068ccf5678",
        "id": "qu_r57M0ZKBH"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9945433831024765"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 2"
      ],
      "metadata": {
        "id": "Icohx7oupw65"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Numer of layers = 2"
      ],
      "metadata": {
        "id": "fVRTZJyjZZ18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from numpy.linalg import norm\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "def argmax(vec):\n",
        "    # return the argmax as a python int\n",
        "    _, idx = torch.max(vec, 1)\n",
        "    return idx.item()\n",
        "\n",
        "\n",
        "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
        "def log_sum_exp(vec):\n",
        "    max_score = vec[0, argmax(vec)]\n",
        "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "    return max_score + \\\n",
        "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
        "\n",
        "class BiLSTM_CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim, wv_embedding_dim, method):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "        self.wv_embedding_dim = wv_embedding_dim\n",
        "        self.method = method\n",
        "        self.word_embeds = nn.Embedding(vocab_size, wv_embedding_dim)\n",
        "        self.word_embeds.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "        \n",
        "        self.pos_embeds = nn.Embedding(pos_embedding.shape[0], pos_embedding.shape[0])\n",
        "        self.pos_embeds.weight.data.copy_(torch.from_numpy(pos_embedding))\n",
        "        \n",
        "        self.dep_embeds = nn.Embedding(dep_embedding.shape[0], dep_embedding.shape[0])\n",
        "        self.dep_embeds.weight.data.copy_(torch.from_numpy(dep_embedding))\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
        "                            num_layers=2, bidirectional=True,dropout=0.5)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim*2, self.tagset_size)\n",
        "\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.hidden_initial()\n",
        "        self.dropout_lstm=nn.Dropout(p=0.5)\n",
        "        \n",
        "    def hidden_initial(self):\n",
        "        return (torch.randn(4, 1, self.hidden_dim // 2).to(device), ### different layers ÈúÄË¶ÅÊîπ\n",
        "                torch.randn(4, 1, self.hidden_dim // 2).to(device)) ###\n",
        "\n",
        "    def _forward_alg(self, feats):\n",
        "        # Do the forward algorithm to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "        #print('_forward_alg')\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of\n",
        "                # the previous tag\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to\n",
        "                # next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the\n",
        "                # edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the\n",
        "                # scores.\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "    # method = dot\n",
        "    def attention(self,lstm_out,hidden_out, hidden_dim, method):\n",
        "        #print('attention')\n",
        "        #h_out_T = torch.transpose(hidden_out,0,1)\n",
        "        h_out_T = torch.transpose(hidden_out,1,2)\n",
        "        \n",
        "        h_out_T_1 = h_out_T   \n",
        "        h_out_0 = hidden_out   \n",
        "        for i in range(lstm_out.size()[0]-1):\n",
        "          h_out_T = torch.cat((h_out_T, h_out_T_1), 0)\n",
        "          hidden_out = torch.cat((hidden_out, h_out_0), 0)\n",
        "        W = nn.Linear(hidden_dim,hidden_dim,bias=False)\n",
        "        if method == 'dot':\n",
        "            attention_w = F.softmax(torch.bmm(lstm_out, h_out_T),dim=-1)\n",
        "        elif method == 'scaled-dot':\n",
        "            attention_w = F.softmax(torch.bmm(lstm_out, h_out_T)/np.sqrt(self.hidden_dim),dim=-1)\n",
        "        elif method == 'cosine':\n",
        "            attention_w = F.softmax(torch.bmm(lstm_out,h_out_T)/(torch.norm(lstm_out)*torch.norm(h_out_T)),dim=-1)\n",
        "\n",
        "        elif method == 'general':\n",
        "            attention_w = F.softmax(torch.bmm(lstm_out, W(h_out_T), h_out_T),dim=-1)\n",
        "\n",
        "        attention_output = torch.bmm(attention_w, hidden_out)\n",
        "        concat_output = torch.cat((attention_output, lstm_out), 1)\n",
        "        return concat_output\n",
        "\n",
        "    def lstm_ft_get(self, sentence, pos, dep, method):\n",
        "        self.hidden = self.hidden_initial()\n",
        "        embeds = torch.cat((self.word_embeds(sentence).view(len(sentence), 1, -1),self.pos_embeds(pos).view(len(pos), 1, -1),self.dep_embeds(dep).view(len(dep), 1, -1)),2)\n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        #print('lstm_out.shape',lstm_out.shape)\n",
        "        hidden_out = torch.cat((self.hidden[0].view(2, 2, 1, -1)[1,0,:,:],self.hidden[0].view(2, 2, 1, -1)[1,1,:,:]),1) ###################‰∏çÂêålayerËøôÈáåÈúÄË¶ÅÊîπ\n",
        "        hidden_out = hidden_out.unsqueeze(0)\n",
        "        #print('hidden_out.shape',hidden_out.shape)\n",
        "        attention_out = self.attention(lstm_out,hidden_out,hidden_dim,method)\n",
        "        attention_out = attention_out.view(-1,self.hidden_dim*2)\n",
        "        #print('attention_out.shape',attention_out.shape)\n",
        "        attention_out = self.dropout_lstm(attention_out)\n",
        "        lstm_fts = self.hidden2tag(attention_out)\n",
        "        #print('lstm_fts.shape',lstm_fts.shape)\n",
        "        return lstm_fts\n",
        "\n",
        "    def sentence_scoring(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1).to(device)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long).to(device), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + \\\n",
        "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def viterbi_decoding(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = []  # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                # We don't include the emission scores here because the max\n",
        "                # does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set\n",
        "            # of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "    def neg_ll(self, sentence, tags, pos, dep, method):\n",
        "        feats = self.lstm_ft_get(sentence, pos, dep, method)\n",
        "        #print('neg_ll')\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self.sentence_scoring(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    #def forward(self, sentence, ent, pos, dep, bert):  # dont confuse this with _forward_alg above.\n",
        "    def forward(self, sentence, pos, dep):\n",
        "        #print('forward')\n",
        "        # Get the emission scores from the BiLSTM\n",
        "\n",
        "        lstm_fts = self.lstm_ft_get(sentence, pos, dep, method)\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self.viterbi_decoding(lstm_fts)\n",
        "        return score, tag_seq\n",
        "\n",
        "\n",
        "\n",
        "def accuracy_calculation(model, input_index, output_index, pos_index, dep_index):\n",
        "\n",
        "  c, total = 0, 0\n",
        "\n",
        "  pred_list, truth = [], []\n",
        "\n",
        "  #print('accuracy_calculation')\n",
        "  for i in range(len(input_index)):\n",
        "    sentence_in = torch.tensor(input_index[i], dtype=torch.long).to(device)\n",
        "    \n",
        "    pos = torch.tensor(pos_index[i], dtype=torch.long).to(device)\n",
        "    dep = torch.tensor(dep_index[i], dtype=torch.long).to(device)\n",
        "\n",
        "\n",
        "\n",
        "    _, pred = model.forward(sentence_in, pos, dep)\n",
        "    for j in range(len(output_index[i])):\n",
        "      total += 1\n",
        "      \n",
        "      pred_list.append(pred[j])\n",
        "      truth.append(output_index[i][j])\n",
        "\n",
        "      if pred[j] == output_index[i][j]:\n",
        "        c += 1\n",
        "\n",
        "  accuracy = c/total\n",
        "\n",
        "  return truth, pred_list, accuracy\n"
      ],
      "metadata": {
        "id": "puJyHLAFbKBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "hidden_dim = 250\n",
        "\n",
        "model = BiLSTM_CRF(len(full_word_to_ix), tag_to_ix, embedding_dim, hidden_dim, WV_EMBEDDING_DIM, method='scaled-dot').to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, weight_decay=1e-4)\n",
        "\n",
        "import datetime\n",
        "for epoch in range(2):  \n",
        "    time1 = datetime.datetime.now()\n",
        "    train_loss = 0\n",
        "    method='scaled-dot'\n",
        "    model.train()\n",
        "    for i in range(len(train_input_index)):\n",
        "        #print(i)\n",
        "        tags_index = train_output_index[i]\n",
        "        dep_index = train_dep_index[i]\n",
        "        pos_index = train_pos_index[i]\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is,\n",
        "        # turn them into Tensors of word indices.\n",
        "        sentence_in = torch.tensor(train_input_index[i], dtype=torch.long).to(device)\n",
        "        pos = torch.tensor(pos_index, dtype=torch.long).to(device)\n",
        "        dep = torch.tensor(dep_index, dtype=torch.long).to(device)\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "        # Step 3. Run our forward pass.\n",
        "        loss = model.neg_ll(sentence_in, targets, pos, dep, method='scaled-dot')\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        # calling optimizer.step()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss+=loss.item()\n",
        "\n",
        "    model.eval()\n",
        "     # \n",
        "    _, _, train_acc = accuracy_calculation(model,train_input_index,train_output_index,train_pos_index,train_dep_index)\n",
        "    _, _, val_acc = accuracy_calculation(model,val_input_index,val_output_index,val_pos_index,val_dep_index)\n",
        "\n",
        "    val_loss = 0\n",
        "    for i in range(len(val_input_index)):\n",
        "        tags_index = val_output_index[i]\n",
        "        pos_index = val_pos_index[i]\n",
        "        dep_index = val_dep_index[i]\n",
        "        sentence_in = torch.tensor(val_input_index[i], dtype=torch.long).to(device)\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "        pos = torch.tensor(pos_index, dtype=torch.long).to(device)\n",
        "        dep = torch.tensor(dep_index, dtype=torch.long).to(device)\n",
        "        loss = model.neg_ll(sentence_in, targets, pos, dep, method='scaled-dot')\n",
        "        val_loss+=loss.item()\n",
        "    time2 = datetime.datetime.now()\n",
        "\n",
        "    print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))\n",
        "\n",
        "y_pred, y_true, _ = accuracy_calculation(model,val_input_index,val_output_index,val_pos_index,val_dep_index)\n",
        "def decode_output(output_list):\n",
        "    ix_to_tag = {v:k for k,v in tag_to_ix.items()}\n",
        "    return [ix_to_tag[output] for output in output_list]\n",
        "y_true_decode = decode_output(y_true)\n",
        "y_pred_decode = decode_output(y_pred)\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true_decode,y_pred_decode,digits=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12de2a40-d400-4a95-84b6-f4074541029b",
        "id": "J5MwIpllbKBv"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:1, Training loss: 34709.38, train acc: 0.9865, val loss: 2175.60, val acc: 0.9844, time: 630.77s\n",
            "Epoch:2, Training loss: 5511.21, train acc: 0.9958, val loss: 1519.26, val acc: 0.9933, time: 619.77s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9677    0.9802    0.9739      1620\n",
            "           D     0.9045    0.9863    0.9436       365\n",
            "           O     0.9997    0.9921    0.9959     19130\n",
            "           P     0.9970    0.9997    0.9983      3925\n",
            "           S     0.9831    0.9973    0.9901      3275\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.9598    0.9819    0.9707      1436\n",
            "\n",
            "    accuracy                         0.9933     33354\n",
            "   macro avg     0.9731    0.9911    0.9818     33354\n",
            "weighted avg     0.9935    0.9933    0.9933     33354\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.metrics import f1_score\n",
        "sklearn.metrics.f1_score(y_true_decode, y_pred_decode,average='micro')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ce552a6-27ba-4e4f-d5aa-9099bcb9af19",
        "id": "lu4Ao5Q5bKBw"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9932841638184325"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 3"
      ],
      "metadata": {
        "id": "ZrHaCv91Zhgo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of layers = 3"
      ],
      "metadata": {
        "id": "qGe6Zqr3b4CI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from numpy.linalg import norm\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "def argmax(vec):\n",
        "    # return the argmax as a python int\n",
        "    _, idx = torch.max(vec, 1)\n",
        "    return idx.item()\n",
        "\n",
        "\n",
        "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
        "def log_sum_exp(vec):\n",
        "    max_score = vec[0, argmax(vec)]\n",
        "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "    return max_score + \\\n",
        "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
        "\n",
        "class BiLSTM_CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim, wv_embedding_dim, method):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "        self.wv_embedding_dim = wv_embedding_dim\n",
        "        self.method = method\n",
        "        self.word_embeds = nn.Embedding(vocab_size, wv_embedding_dim)\n",
        "        self.word_embeds.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "        \n",
        "        self.pos_embeds = nn.Embedding(pos_embedding.shape[0], pos_embedding.shape[0])\n",
        "        self.pos_embeds.weight.data.copy_(torch.from_numpy(pos_embedding))\n",
        "        \n",
        "        self.dep_embeds = nn.Embedding(dep_embedding.shape[0], dep_embedding.shape[0])\n",
        "        self.dep_embeds.weight.data.copy_(torch.from_numpy(dep_embedding))\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
        "                            num_layers=3, bidirectional=True,dropout=0.5)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim*2, self.tagset_size)\n",
        "\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.hidden_initial()\n",
        "        self.dropout_lstm=nn.Dropout(p=0.5)\n",
        "        \n",
        "    def hidden_initial(self):\n",
        "        return (torch.randn(6, 1, self.hidden_dim // 2).to(device), ### different layers ÈúÄË¶ÅÊîπ\n",
        "                torch.randn(6, 1, self.hidden_dim // 2).to(device)) ###\n",
        "\n",
        "    def _forward_alg(self, feats):\n",
        "        # Do the forward algorithm to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "        #print('_forward_alg')\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of\n",
        "                # the previous tag\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to\n",
        "                # next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the\n",
        "                # edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the\n",
        "                # scores.\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "    # method = dot\n",
        "    def attention(self,lstm_out,hidden_out, hidden_dim, method):\n",
        "\n",
        "        h_out_T = torch.transpose(hidden_out,1,2)\n",
        "        \n",
        "        h_out_T_1 = h_out_T   \n",
        "        h_out_0 = hidden_out   \n",
        "        for i in range(lstm_out.size()[0]-1):\n",
        "          h_out_T = torch.cat((h_out_T, h_out_T_1), 0)\n",
        "          hidden_out = torch.cat((hidden_out, h_out_0), 0)\n",
        "        W = nn.Linear(hidden_dim,hidden_dim,bias=False)\n",
        "        if method == 'dot':\n",
        "            attention_w = F.softmax(torch.bmm(lstm_out, h_out_T),dim=-1)\n",
        "        elif method == 'scaled-dot':\n",
        "            attention_w = F.softmax(torch.bmm(lstm_out, h_out_T)/np.sqrt(self.hidden_dim),dim=-1)\n",
        "        elif method == 'cosine':\n",
        "            attention_w = F.softmax(torch.bmm(lstm_out,h_out_T)/(torch.norm(lstm_out)*torch.norm(h_out_T)),dim=-1)\n",
        "\n",
        "        elif method == 'general':\n",
        "            attention_w = F.softmax(torch.bmm(lstm_out, W(h_out_T), h_out_T),dim=-1)\n",
        "\n",
        "        attention_output = torch.bmm(attention_w, hidden_out)\n",
        "        concat_output = torch.cat((attention_output, lstm_out), 1)\n",
        "        return concat_output\n",
        "\n",
        "    def lstm_ft_get(self, sentence, pos, dep, method):\n",
        "        self.hidden = self.hidden_initial()\n",
        "        embeds = torch.cat((self.word_embeds(sentence).view(len(sentence), 1, -1),self.pos_embeds(pos).view(len(pos), 1, -1),self.dep_embeds(dep).view(len(dep), 1, -1)),2)\n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        #print('lstm_out.shape',lstm_out.shape)\n",
        "        hidden_out = torch.cat((self.hidden[0].view(3, 2, 1, -1)[1,0,:,:],self.hidden[0].view(3, 2, 1, -1)[1,1,:,:]),1) ###################‰∏çÂêålayerËøôÈáåÈúÄË¶ÅÊîπ\n",
        "        hidden_out = hidden_out.unsqueeze(0)\n",
        "        #print('hidden_out.shape',hidden_out.shape)\n",
        "        attention_out = self.attention(lstm_out,hidden_out,hidden_dim,method)\n",
        "        attention_out = attention_out.view(-1,self.hidden_dim*2)\n",
        "        #print('attention_out.shape',attention_out.shape)\n",
        "        attention_out = self.dropout_lstm(attention_out)\n",
        "        lstm_fts = self.hidden2tag(attention_out)\n",
        "        #print('lstm_fts.shape',lstm_fts.shape)\n",
        "        return lstm_fts\n",
        "\n",
        "    def sentence_scoring(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1).to(device)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long).to(device), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + \\\n",
        "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def viterbi_decoding(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = []  # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                # We don't include the emission scores here because the max\n",
        "                # does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set\n",
        "            # of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "    def neg_ll(self, sentence, tags, pos, dep, method):\n",
        "        feats = self.lstm_ft_get(sentence, pos, dep, method)\n",
        "        #print('neg_ll')\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self.sentence_scoring(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    #def forward(self, sentence, ent, pos, dep, bert):  # dont confuse this with _forward_alg above.\n",
        "    def forward(self, sentence, pos, dep):\n",
        "        #print('forward')\n",
        "        # Get the emission scores from the BiLSTM\n",
        "\n",
        "        lstm_fts = self.lstm_ft_get(sentence, pos, dep, method)\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self.viterbi_decoding(lstm_fts)\n",
        "        return score, tag_seq\n",
        "\n",
        "\n",
        "\n",
        "def accuracy_calculation(model, input_index, output_index, pos_index, dep_index):\n",
        "\n",
        "  c, total = 0, 0\n",
        "\n",
        "  pred_list, truth = [], []\n",
        "\n",
        "  #print('accuracy_calculation')\n",
        "for i in range(len(input_index)):\n",
        "    sentence_in = torch.tensor(input_index[i], dtype=torch.long).to(device)\n",
        "    \n",
        "    pos = torch.tensor(pos_index[i], dtype=torch.long).to(device)\n",
        "    dep = torch.tensor(dep_index[i], dtype=torch.long).to(device)\n",
        "\n",
        "\n",
        "\n",
        "    _, pred = model.forward(sentence_in, pos, dep)\n",
        "    for j in range(len(output_index[i])):\n",
        "      total += 1\n",
        "      \n",
        "      pred_list.append(pred[j])\n",
        "      truth.append(output_index[i][j])\n",
        "\n",
        "      if pred[j] == output_index[i][j]:\n",
        "        c += 1\n",
        "\n",
        "  accuracy = c/total\n",
        "\n",
        "  return truth, pred_list, accuracy\n"
      ],
      "metadata": {
        "id": "5NFrE_VqZtf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "hidden_dim = 250\n",
        "\n",
        "model = BiLSTM_CRF(len(full_word_to_ix), tag_to_ix, embedding_dim, hidden_dim, WV_EMBEDDING_DIM, method='scaled-dot').to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, weight_decay=1e-4)\n",
        "\n",
        "import datetime\n",
        "for epoch in range(2):  \n",
        "    time1 = datetime.datetime.now()\n",
        "    train_loss = 0\n",
        "    method='scaled-dot'\n",
        "    model.train()\n",
        "    for i in range(len(train_input_index)):\n",
        "        #print(i)\n",
        "        tags_index = train_output_index[i]\n",
        "        dep_index = train_dep_index[i]\n",
        "        pos_index = train_pos_index[i]\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is,\n",
        "        # turn them into Tensors of word indices.\n",
        "        sentence_in = torch.tensor(train_input_index[i], dtype=torch.long).to(device)\n",
        "        pos = torch.tensor(pos_index, dtype=torch.long).to(device)\n",
        "        dep = torch.tensor(dep_index, dtype=torch.long).to(device)\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "        # Step 3. Run our forward pass.\n",
        "        loss = model.neg_ll(sentence_in, targets, pos, dep, method='scaled-dot')\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        # calling optimizer.step()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss+=loss.item()\n",
        "\n",
        "    model.eval()\n",
        "     # \n",
        "    _, _, train_acc = accuracy_calculation(model,train_input_index,train_output_index,train_pos_index,train_dep_index)\n",
        "    _, _, val_acc = accuracy_calculation(model,val_input_index,val_output_index,val_pos_index,val_dep_index)\n",
        "\n",
        "    val_loss = 0\n",
        "    for i in range(len(val_input_index)):\n",
        "        tags_index = val_output_index[i]\n",
        "        pos_index = val_pos_index[i]\n",
        "        dep_index = val_dep_index[i]\n",
        "        sentence_in = torch.tensor(val_input_index[i], dtype=torch.long).to(device)\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "        pos = torch.tensor(pos_index, dtype=torch.long).to(device)\n",
        "        dep = torch.tensor(dep_index, dtype=torch.long).to(device)\n",
        "        loss = model.neg_ll(sentence_in, targets, pos, dep, method='scaled-dot')\n",
        "        val_loss+=loss.item()\n",
        "    time2 = datetime.datetime.now()\n",
        "\n",
        "    print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))\n",
        "\n",
        "y_pred, y_true, _ = accuracy_calculation(model,val_input_index,val_output_index,val_pos_index,val_dep_index)\n",
        "def decode_output(output_list):\n",
        "    ix_to_tag = {v:k for k,v in tag_to_ix.items()}\n",
        "    return [ix_to_tag[output] for output in output_list]\n",
        "y_true_decode = decode_output(y_true)\n",
        "y_pred_decode = decode_output(y_pred)\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true_decode,y_pred_decode,digits=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b6d57b3-e3d0-4abb-a348-c675f2316ab1",
        "id": "05MFoe5kZtf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:1, Training loss: 63081.22, train acc: 0.9528, val loss: 6157.26, val acc: 0.9524, time: 664.89s\n",
            "Epoch:2, Training loss: 16728.59, train acc: 0.9836, val loss: 2717.06, val acc: 0.9822, time: 667.52s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9366    0.9505    0.9435      1617\n",
            "           D     0.6734    0.7813    0.7233       343\n",
            "           O     0.9981    0.9848    0.9914     19240\n",
            "           P     0.9863    0.9992    0.9927      3885\n",
            "           S     0.9600    0.9953    0.9773      3204\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.9176    0.9220    0.9198      1462\n",
            "\n",
            "    accuracy                         0.9826     33354\n",
            "   macro avg     0.9246    0.9476    0.9354     33354\n",
            "weighted avg     0.9834    0.9826    0.9829     33354\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.metrics import f1_score\n",
        "sklearn.metrics.f1_score(y_true_decode, y_pred_decode,average='micro')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2839c0bf-4727-4a18-ccec-eb1e63fb75aa",
        "id": "wAMwkBneZtf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9826407627271092"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of layers = 3"
      ],
      "metadata": {
        "id": "F62b6LsNZjaI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4. Ablation Study 4: with/without CRF"
      ],
      "metadata": {
        "id": "1q12-hN3spYA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 1 With CRF"
      ],
      "metadata": {
        "id": "OeRi_7gpOCOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from numpy.linalg import norm\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "def argmax(vec):\n",
        "    # return the argmax as a python int\n",
        "    _, idx = torch.max(vec, 1)\n",
        "    return idx.item()\n",
        "\n",
        "\n",
        "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
        "def log_sum_exp(vec):\n",
        "    max_score = vec[0, argmax(vec)]\n",
        "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "    return max_score + \\\n",
        "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
        "\n",
        "class BiLSTM_CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim, wv_embedding_dim, method):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "        self.wv_embedding_dim = wv_embedding_dim\n",
        "        self.method = method\n",
        "        self.word_embeds = nn.Embedding(vocab_size, wv_embedding_dim)\n",
        "        self.word_embeds.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "        \n",
        "        self.pos_embeds = nn.Embedding(pos_embedding.shape[0], pos_embedding.shape[0])\n",
        "        self.pos_embeds.weight.data.copy_(torch.from_numpy(pos_embedding))\n",
        "        \n",
        "        self.dep_embeds = nn.Embedding(dep_embedding.shape[0], dep_embedding.shape[0])\n",
        "        self.dep_embeds.weight.data.copy_(torch.from_numpy(dep_embedding))\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
        "                            num_layers=1, bidirectional=True,dropout=0.5)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim*2, self.tagset_size)\n",
        "\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.hidden_initial()\n",
        "        self.dropout_lstm=nn.Dropout(p=0.5)\n",
        "        \n",
        "    def hidden_initial(self):\n",
        "        return (torch.randn(2, 1, self.hidden_dim // 2).to(device), ### different layers ÈúÄË¶ÅÊîπ\n",
        "                torch.randn(2, 1, self.hidden_dim // 2).to(device)) ###\n",
        "\n",
        "    def _forward_alg(self, feats):\n",
        "        # Do the forward algorithm to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "        #print('_forward_alg')\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of\n",
        "                # the previous tag\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to\n",
        "                # next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the\n",
        "                # edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the\n",
        "                # scores.\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "    # method = dot\n",
        "    def attention(self,lstm_out,hidden_out, hidden_dim, method):\n",
        "\n",
        "        h_out_T = torch.transpose(hidden_out,1,2)\n",
        "        \n",
        "        h_out_T_1 = h_out_T   \n",
        "        h_out_0 = hidden_out   \n",
        "        for i in range(lstm_out.size()[0]-1):\n",
        "          h_out_T = torch.cat((h_out_T, h_out_T_1), 0)\n",
        "          hidden_out = torch.cat((hidden_out, h_out_0), 0)\n",
        "        W = nn.Linear(hidden_dim,hidden_dim,bias=False)\n",
        "        if method == 'dot':\n",
        "            attention_w = F.softmax(torch.bmm(lstm_out, h_out_T),dim=-1)\n",
        "        elif method == 'scaled-dot':\n",
        "            attention_w = F.softmax(torch.bmm(lstm_out, h_out_T)/np.sqrt(self.hidden_dim),dim=-1)\n",
        "        elif method == 'cosine':\n",
        "            attention_w = F.softmax(torch.bmm(lstm_out,h_out_T)/(torch.norm(lstm_out)*torch.norm(h_out_T)),dim=-1)\n",
        "\n",
        "        elif method == 'general':\n",
        "            attention_w = F.softmax(torch.bmm(lstm_out, W(h_out_T), h_out_T),dim=-1)\n",
        "\n",
        "        attention_output = torch.bmm(attention_w, hidden_out)\n",
        "        concat_output = torch.cat((attention_output, lstm_out), 1)\n",
        "        return concat_output\n",
        "\n",
        "    def lstm_ft_get(self, sentence, pos, dep, method):\n",
        "        self.hidden = self.hidden_initial()\n",
        "        embeds = torch.cat((self.word_embeds(sentence).view(len(sentence), 1, -1),self.pos_embeds(pos).view(len(pos), 1, -1),self.dep_embeds(dep).view(len(dep), 1, -1)),2)\n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        #print('lstm_out.shape',lstm_out.shape)\n",
        "        hidden_out = torch.cat((self.hidden[0].view(1, 2, 1, -1)[0,0,:,:],self.hidden[0].view(1, 2, 1, -1)[0,1,:,:]),1) ###################‰∏çÂêålayerËøôÈáåÈúÄË¶ÅÊîπ\n",
        "        hidden_out = hidden_out.unsqueeze(0)\n",
        "        #print('hidden_out.shape',hidden_out.shape)\n",
        "        attention_out = self.attention(lstm_out,hidden_out,hidden_dim,method)\n",
        "        attention_out = attention_out.view(-1,self.hidden_dim*2)\n",
        "        #print('attention_out.shape',attention_out.shape)\n",
        "        attention_out = self.dropout_lstm(attention_out)\n",
        "        lstm_fts = self.hidden2tag(attention_out)\n",
        "        #print('lstm_fts.shape',lstm_fts.shape)\n",
        "        return lstm_fts\n",
        "\n",
        "    def sentence_scoring(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1).to(device)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long).to(device), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + \\\n",
        "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def viterbi_decoding(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = []  # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                # We don't include the emission scores here because the max\n",
        "                # does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set\n",
        "            # of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "    def neg_ll(self, sentence, tags, pos, dep, method):\n",
        "        feats = self.lstm_ft_get(sentence, pos, dep, method)\n",
        "        #print('neg_ll')\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self.sentence_scoring(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    #def forward(self, sentence, ent, pos, dep, bert):  # dont confuse this with _forward_alg above.\n",
        "    def forward(self, sentence, pos, dep):\n",
        "        #print('forward')\n",
        "        # Get the emission scores from the BiLSTM\n",
        "\n",
        "        lstm_fts = self.lstm_ft_get(sentence, pos, dep, method)\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self.viterbi_decoding(lstm_fts)\n",
        "        return score, tag_seq\n",
        "\n",
        "\n",
        "\n",
        "def accuracy_calculation(model, input_index, output_index, pos_index, dep_index):\n",
        "\n",
        "  c, total = 0, 0\n",
        "\n",
        "  pred_list, truth = [], []\n",
        "\n",
        "  #print('accuracy_calculation')\n",
        "  for i in range(len(input_index)):\n",
        "    sentence_in = torch.tensor(input_index[i], dtype=torch.long).to(device)\n",
        "    \n",
        "    pos = torch.tensor(pos_index[i], dtype=torch.long).to(device)\n",
        "    dep = torch.tensor(dep_index[i], dtype=torch.long).to(device)\n",
        "\n",
        "\n",
        "\n",
        "    _, pred = model.forward(sentence_in, pos, dep)\n",
        "    for j in range(len(output_index[i])):\n",
        "      total += 1\n",
        "      \n",
        "      pred_list.append(pred[j])\n",
        "      truth.append(output_index[i][j])\n",
        "\n",
        "      if pred[j] == output_index[i][j]:\n",
        "        c += 1\n",
        "\n",
        "  accuracy = c/total\n",
        "\n",
        "  return truth, pred_list, accuracy\n"
      ],
      "metadata": {
        "id": "dwv7qGUJOJvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "hidden_dim = 250\n",
        "\n",
        "model = BiLSTM_CRF(len(full_word_to_ix), tag_to_ix, embedding_dim, hidden_dim, WV_EMBEDDING_DIM, method='scaled-dot').to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, weight_decay=1e-4)\n",
        "\n",
        "import datetime\n",
        "for epoch in range(2):  \n",
        "    time1 = datetime.datetime.now()\n",
        "    train_loss = 0\n",
        "    method='scaled-dot'\n",
        "    model.train()\n",
        "    for i in range(len(train_input_index)):\n",
        "        #print(i)\n",
        "        tags_index = train_output_index[i]\n",
        "        dep_index = train_dep_index[i]\n",
        "        pos_index = train_pos_index[i]\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is,\n",
        "        # turn them into Tensors of word indices.\n",
        "        sentence_in = torch.tensor(train_input_index[i], dtype=torch.long).to(device)\n",
        "        pos = torch.tensor(pos_index, dtype=torch.long).to(device)\n",
        "        dep = torch.tensor(dep_index, dtype=torch.long).to(device)\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "        # Step 3. Run our forward pass.\n",
        "        loss = model.neg_ll(sentence_in, targets, pos, dep, method='scaled-dot')\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        # calling optimizer.step()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss+=loss.item()\n",
        "\n",
        "    model.eval()\n",
        "     # \n",
        "    _, _, train_acc = accuracy_calculation(model,train_input_index,train_output_index,train_pos_index,train_dep_index)\n",
        "    _, _, val_acc = accuracy_calculation(model,val_input_index,val_output_index,val_pos_index,val_dep_index)\n",
        "\n",
        "    val_loss = 0\n",
        "    for i in range(len(val_input_index)):\n",
        "        tags_index = val_output_index[i]\n",
        "        pos_index = val_pos_index[i]\n",
        "        dep_index = val_dep_index[i]\n",
        "        sentence_in = torch.tensor(val_input_index[i], dtype=torch.long).to(device)\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "        pos = torch.tensor(pos_index, dtype=torch.long).to(device)\n",
        "        dep = torch.tensor(dep_index, dtype=torch.long).to(device)\n",
        "        loss = model.neg_ll(sentence_in, targets, pos, dep, method='scaled-dot')\n",
        "        val_loss+=loss.item()\n",
        "    time2 = datetime.datetime.now()\n",
        "\n",
        "    print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))\n",
        "\n",
        "y_pred, y_true, _ = accuracy_calculation(model,val_input_index,val_output_index,val_pos_index,val_dep_index)\n",
        "def decode_output(output_list):\n",
        "    ix_to_tag = {v:k for k,v in tag_to_ix.items()}\n",
        "    return [ix_to_tag[output] for output in output_list]\n",
        "y_true_decode = decode_output(y_true)\n",
        "y_pred_decode = decode_output(y_pred)\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true_decode,y_pred_decode,digits=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e88ee77-21fb-43b6-a5d2-09336737ab20",
        "id": "Z1wkg6AzOJva"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:1, Training loss: 24818.84, train acc: 0.9912, val loss: 1663.70, val acc: 0.9886, time: 611.66s\n",
            "Epoch:2, Training loss: 3723.22, train acc: 0.9979, val loss: 1371.56, val acc: 0.9946, time: 619.91s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9707    0.9956    0.9830      1600\n",
            "           D     0.9296    0.9763    0.9524       379\n",
            "           O     0.9997    0.9926    0.9961     19120\n",
            "           P     0.9982    0.9995    0.9989      3931\n",
            "           S     0.9898    0.9994    0.9946      3290\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.9598    0.9853    0.9724      1431\n",
            "\n",
            "    accuracy                         0.9945     33354\n",
            "   macro avg     0.9783    0.9927    0.9853     33354\n",
            "weighted avg     0.9947    0.9945    0.9946     33354\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.metrics import f1_score\n",
        "sklearn.metrics.f1_score(y_true_decode, y_pred_decode,average='micro')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa713f8f-4e35-4053-a5ba-2c068ccf5678",
        "id": "_Jz3D98WOJva"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9945433831024765"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 2 Without CRF"
      ],
      "metadata": {
        "id": "SfwcYA4LAn8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from numpy.linalg import norm\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "\n",
        "\n",
        "class BiLSTM_CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim, wv_embedding_dim, method):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "        self.wv_embedding_dim = wv_embedding_dim\n",
        "        self.method = method\n",
        "        self.word_embeds = nn.Embedding(vocab_size, wv_embedding_dim)\n",
        "        self.word_embeds.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "        \n",
        "        self.pos_embeds = nn.Embedding(pos_embedding.shape[0], pos_embedding.shape[0])\n",
        "        self.pos_embeds.weight.data.copy_(torch.from_numpy(pos_embedding))\n",
        "        \n",
        "        self.dep_embeds = nn.Embedding(dep_embedding.shape[0], dep_embedding.shape[0])\n",
        "        self.dep_embeds.weight.data.copy_(torch.from_numpy(dep_embedding))\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
        "                            num_layers=1, bidirectional=True,dropout=0.5)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim*2, self.tagset_size)\n",
        "\n",
        "\n",
        "        self.hidden = self.hidden_initial()\n",
        "        self.dropout_lstm=nn.Dropout(p=0.5)\n",
        "        \n",
        "    def hidden_initial(self):\n",
        "        return (torch.randn(2, 1, self.hidden_dim // 2).to(device), ### different layers ÈúÄË¶ÅÊîπ\n",
        "                torch.randn(2, 1, self.hidden_dim // 2).to(device)) ###\n",
        "\n",
        "\n",
        "    # method = dot\n",
        "    def attention(self,lstm_out,hidden_out, hidden_dim, method):\n",
        "\n",
        "        h_out_T = torch.transpose(hidden_out,1,2)\n",
        "        \n",
        "        h_out_T_1 = h_out_T   \n",
        "        h_out_0 = hidden_out   \n",
        "        for i in range(lstm_out.size()[0]-1):\n",
        "          h_out_T = torch.cat((h_out_T, h_out_T_1), 0)\n",
        "          hidden_out = torch.cat((hidden_out, h_out_0), 0)\n",
        "        W = nn.Linear(hidden_dim,hidden_dim,bias=False)\n",
        "        if method == 'dot':\n",
        "            attention_w = F.softmax(torch.bmm(lstm_out, h_out_T),dim=-1)\n",
        "        elif method == 'scaled-dot':\n",
        "            attention_w = F.softmax(torch.bmm(lstm_out, h_out_T)/np.sqrt(self.hidden_dim),dim=-1)\n",
        "        elif method == 'cosine':\n",
        "            attention_w = F.softmax(torch.bmm(lstm_out,h_out_T)/(torch.norm(lstm_out)*torch.norm(h_out_T)),dim=-1)\n",
        "\n",
        "        elif method == 'general':\n",
        "            attention_w = F.softmax(torch.bmm(lstm_out, W(h_out_T), h_out_T),dim=-1)\n",
        "\n",
        "        attention_output = torch.bmm(attention_w, hidden_out)\n",
        "        concat_output = torch.cat((attention_output, lstm_out), 1)\n",
        "        return concat_output\n",
        "\n",
        "\n",
        "    def forward(self, sentence, pos, dep, method):\n",
        "        self.hidden = self.hidden_initial()\n",
        "        embeds = torch.cat((self.word_embeds(sentence).view(len(sentence), 1, -1),self.pos_embeds(pos).view(len(pos), 1, -1),self.dep_embeds(dep).view(len(dep), 1, -1)),2)\n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        #print('lstm_out.shape',lstm_out.shape)\n",
        "        hidden_out = torch.cat((self.hidden[0].view(1, 2, 1, -1)[0,0,:,:],self.hidden[0].view(1, 2, 1, -1)[0,1,:,:]),1) ###################‰∏çÂêålayerËøôÈáåÈúÄË¶ÅÊîπ\n",
        "        hidden_out = hidden_out.unsqueeze(0)\n",
        "        #print('hidden_out.shape',hidden_out.shape)\n",
        "        attention_out = self.attention(lstm_out,hidden_out,hidden_dim,method)\n",
        "        attention_out = attention_out.view(-1,self.hidden_dim*2)\n",
        "        #print('attention_out.shape',attention_out.shape)\n",
        "        attention_out = self.dropout_lstm(attention_out)\n",
        "        lstm_fts = self.hidden2tag(attention_out)\n",
        "        #print('lstm_fts.shape',lstm_fts.shape)\n",
        "        return lstm_fts\n",
        "\n",
        "   \n",
        "\n",
        "    def neg_ll(self, sentence, tags, pos, dep, method):\n",
        "        feats = self.lstm_ft_get(sentence, pos, dep, method)\n",
        "        #print('neg_ll')\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self.sentence_scoring(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def accuracy_calculation(model, input_index, output_index, pos_index, dep_index):\n",
        "\n",
        "  c, total = 0, 0\n",
        "\n",
        "  pred_list, truth = [], []\n",
        "\n",
        "  #print('accuracy_calculation')\n",
        "for i in range(len(input_index)):\n",
        "    sentence_in = torch.tensor(input_index[i], dtype=torch.long).to(device)\n",
        "    \n",
        "    pos = torch.tensor(pos_index[i], dtype=torch.long).to(device)\n",
        "    dep = torch.tensor(dep_index[i], dtype=torch.long).to(device)\n",
        "\n",
        "\n",
        "\n",
        "    output = model.forward(sentence_in, pos, dep, 'scaled-dot')\n",
        "    pred = torch.argmax(output, -1)\n",
        "    for j in range(len(output_index[i])):\n",
        "      total += 1\n",
        "      \n",
        "      pred_list.append(pred[j])\n",
        "      truth.append(output_index[i][j])\n",
        "\n",
        "      if pred[j] == output_index[i][j]:\n",
        "        c += 1\n",
        "\n",
        "  accuracy = c/total\n",
        "\n",
        "  return truth, pred_list, accuracy\n"
      ],
      "metadata": {
        "id": "FUbO2cgIHsmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "hidden_dim = 250\n",
        "\n",
        "model = BiLSTM_CRF(len(full_word_to_ix), tag_to_ix, embedding_dim, hidden_dim, WV_EMBEDDING_DIM, method='scaled-dot').to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, weight_decay=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "import datetime\n",
        "for epoch in range(2):  \n",
        "    time1 = datetime.datetime.now()\n",
        "    train_loss = 0\n",
        "    method='scaled-dot'\n",
        "    model.train()\n",
        "    for i in range(len(train_input_index)):\n",
        "        #print(i)\n",
        "        tags_index = train_output_index[i]\n",
        "        dep_index = train_dep_index[i]\n",
        "        pos_index = train_pos_index[i]\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is,\n",
        "        # turn them into Tensors of word indices.\n",
        "        sentence_in = torch.tensor(train_input_index[i], dtype=torch.long).to(device)\n",
        "        pos = torch.tensor(pos_index, dtype=torch.long).to(device)\n",
        "        dep = torch.tensor(dep_index, dtype=torch.long).to(device)\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "        # Step 3. Run our forward pass.\n",
        "        output = model(sentence_in, pos, dep, 'scaled-dot')\n",
        "\n",
        "        loss = criterion(output, targets) \n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        # calling optimizer.step()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss+=loss.item()\n",
        "\n",
        "    model.eval()\n",
        "     # \n",
        "    _, _, train_acc = accuracy_calculation(model,train_input_index,train_output_index,train_pos_index,train_dep_index)\n",
        "    _, _, val_acc = accuracy_calculation(model,val_input_index,val_output_index,val_pos_index,val_dep_index)\n",
        "\n",
        "    val_loss = 0\n",
        "    for i in range(len(val_input_index)):\n",
        "        tags_index = val_output_index[i]\n",
        "        pos_index = val_pos_index[i]\n",
        "        dep_index = val_dep_index[i]\n",
        "        sentence_in = torch.tensor(val_input_index[i], dtype=torch.long).to(device)\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "        pos = torch.tensor(pos_index, dtype=torch.long).to(device)\n",
        "        dep = torch.tensor(dep_index, dtype=torch.long).to(device)\n",
        "        output = model(sentence_in, pos, dep, 'scaled-dot')\n",
        "        loss = criterion(output, targets) \n",
        "        val_loss+=loss.item()\n",
        "    time2 = datetime.datetime.now()\n",
        "\n",
        "    print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))\n",
        "\n",
        "y_pred, y_true, _ = accuracy_calculation(model,val_input_index,val_output_index,val_pos_index,val_dep_index)\n",
        "y_true =  torch.tensor(y_true, device = 'cpu').numpy().tolist()\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true, y_pred,digits=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7d1c872-4de2-4dfb-8234-0aada8d3d183",
        "id": "YzDn7sWiHsmr"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:1, Training loss: 9268.59, train acc: 0.9730, val loss: 964.20, val acc: 0.9730, time: 168.42s\n",
            "Epoch:2, Training loss: 2539.47, train acc: 0.9899, val loss: 554.88, val acc: 0.9873, time: 160.44s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           2     0.9982    0.9868    0.9925     19203\n",
            "           3     0.9346    0.9628    0.9485      1426\n",
            "           4     0.9906    0.9990    0.9948      3903\n",
            "           5     1.0000    1.0000    1.0000      3603\n",
            "           6     0.9825    0.9942    0.9883      3283\n",
            "           7     0.7111    0.9248    0.8040       306\n",
            "           8     0.9494    0.9558    0.9526      1630\n",
            "\n",
            "    accuracy                         0.9873     33354\n",
            "   macro avg     0.9381    0.9748    0.9544     33354\n",
            "weighted avg     0.9882    0.9873    0.9876     33354\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.metrics import f1_score\n",
        "sklearn.metrics.f1_score(y_true, y_pred,average='micro')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b44da8b-386b-4dad-9682-a96e759a2aee",
        "id": "MjG0sC3hHsmr"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9872878815134617"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.5. Performance Comparison"
      ],
      "metadata": {
        "id": "cdaGN3k5bjSc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Best Model"
      ],
      "metadata": {
        "id": "-O06Ycb6brvG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BiLSTM + CRF + Self Attention(scaled dot-product) + 1 layer <br>\n",
        "Embedding: Fasttext(training+validation data) + pos + dep + fasttext(dota-wiki + CONDA test)"
      ],
      "metadata": {
        "id": "jfvUUPeFoS0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from numpy.linalg import norm\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "def argmax(vec):\n",
        "    # return the argmax as a python int\n",
        "    _, idx = torch.max(vec, 1)\n",
        "    return idx.item()\n",
        "\n",
        "\n",
        "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
        "def log_sum_exp(vec):\n",
        "    max_score = vec[0, argmax(vec)]\n",
        "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "    return max_score + \\\n",
        "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
        "\n",
        "class BiLSTM_CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim, wv_embedding_dim, method):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "        self.wv_embedding_dim = wv_embedding_dim\n",
        "        self.method = method\n",
        "        self.word_embeds = nn.Embedding(vocab_size, wv_embedding_dim)\n",
        "        self.word_embeds.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "        \n",
        "        self.pos_embeds = nn.Embedding(pos_embedding.shape[0], pos_embedding.shape[0])\n",
        "        self.pos_embeds.weight.data.copy_(torch.from_numpy(pos_embedding))\n",
        "        \n",
        "        self.dep_embeds = nn.Embedding(dep_embedding.shape[0], dep_embedding.shape[0])\n",
        "        self.dep_embeds.weight.data.copy_(torch.from_numpy(dep_embedding))\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
        "                            num_layers=1, bidirectional=True,dropout=0.5)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim*2, self.tagset_size)\n",
        "\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.hidden_initial()\n",
        "        self.dropout_lstm=nn.Dropout(p=0.5)\n",
        "        \n",
        "    def hidden_initial(self):\n",
        "        return (torch.randn(2, 1, self.hidden_dim // 2).to(device), ### different layers ÈúÄË¶ÅÊîπ\n",
        "                torch.randn(2, 1, self.hidden_dim // 2).to(device)) ###\n",
        "\n",
        "    def _forward_alg(self, feats):\n",
        "        # Do the forward algorithm to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "        #print('_forward_alg')\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of\n",
        "                # the previous tag\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to\n",
        "                # next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the\n",
        "                # edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the\n",
        "                # scores.\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "    # method = dot\n",
        "    def attention(self,lstm_out,hidden_out, hidden_dim, method):\n",
        "\n",
        "        h_out_T = torch.transpose(hidden_out,1,2)\n",
        "        \n",
        "        h_out_T_1 = h_out_T   \n",
        "        h_out_0 = hidden_out   \n",
        "        for i in range(lstm_out.size()[0]-1):\n",
        "          h_out_T = torch.cat((h_out_T, h_out_T_1), 0)\n",
        "          hidden_out = torch.cat((hidden_out, h_out_0), 0)\n",
        "        W = nn.Linear(hidden_dim,hidden_dim,bias=False)\n",
        "        if method == 'dot':\n",
        "            attention_w = F.softmax(torch.bmm(lstm_out, h_out_T),dim=-1)\n",
        "        elif method == 'scaled-dot':\n",
        "            attention_w = F.softmax(torch.bmm(lstm_out, h_out_T)/np.sqrt(self.hidden_dim),dim=-1)\n",
        "        elif method == 'cosine':\n",
        "            attention_w = F.softmax(torch.bmm(lstm_out,h_out_T)/(torch.norm(lstm_out)*torch.norm(h_out_T)),dim=-1)\n",
        "\n",
        "        elif method == 'general':\n",
        "            attention_w = F.softmax(torch.bmm(lstm_out, W(h_out_T), h_out_T),dim=-1)\n",
        "\n",
        "        attention_output = torch.bmm(attention_w, hidden_out)\n",
        "        concat_output = torch.cat((attention_output, lstm_out), 1)\n",
        "        return concat_output\n",
        "\n",
        "    def lstm_ft_get(self, sentence, pos, dep, method):\n",
        "        self.hidden = self.hidden_initial()\n",
        "        embeds = torch.cat((self.word_embeds(sentence).view(len(sentence), 1, -1),self.pos_embeds(pos).view(len(pos), 1, -1),self.dep_embeds(dep).view(len(dep), 1, -1)),2)\n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        #print('lstm_out.shape',lstm_out.shape)\n",
        "        hidden_out = torch.cat((self.hidden[0].view(1, 2, 1, -1)[0,0,:,:],self.hidden[0].view(1, 2, 1, -1)[0,1,:,:]),1) ###################‰∏çÂêålayerËøôÈáåÈúÄË¶ÅÊîπ\n",
        "        hidden_out = hidden_out.unsqueeze(0)\n",
        "        #print('hidden_out.shape',hidden_out.shape)\n",
        "        attention_out = self.attention(lstm_out,hidden_out,hidden_dim,method)\n",
        "        attention_out = attention_out.view(-1,self.hidden_dim*2)\n",
        "        #print('attention_out.shape',attention_out.shape)\n",
        "        attention_out = self.dropout_lstm(attention_out)\n",
        "        lstm_fts = self.hidden2tag(attention_out)\n",
        "        #print('lstm_fts.shape',lstm_fts.shape)\n",
        "        return lstm_fts\n",
        "\n",
        "    def sentence_scoring(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1).to(device)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long).to(device), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + \\\n",
        "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def viterbi_decoding(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = []  # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                # We don't include the emission scores here because the max\n",
        "                # does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set\n",
        "            # of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "    def neg_ll(self, sentence, tags, pos, dep, method):\n",
        "        feats = self.lstm_ft_get(sentence, pos, dep, method)\n",
        "        #print('neg_ll')\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self.sentence_scoring(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    #def forward(self, sentence, ent, pos, dep, bert):  # dont confuse this with _forward_alg above.\n",
        "    def forward(self, sentence, pos, dep):\n",
        "        #print('forward')\n",
        "        # Get the emission scores from the BiLSTM\n",
        "\n",
        "        lstm_fts = self.lstm_ft_get(sentence, pos, dep, method)\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self.viterbi_decoding(lstm_fts)\n",
        "        return score, tag_seq\n",
        "\n",
        "\n",
        "\n",
        "def accuracy_calculation(model, input_index, output_index, pos_index, dep_index):\n",
        "\n",
        "  c, total = 0, 0\n",
        "\n",
        "  pred_list, truth = [], []\n",
        "\n",
        "  #print('accuracy_calculation')\n",
        "for i in range(len(input_index)):\n",
        "    sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "    \n",
        "    pos = torch.tensor(pos_index[i], dtype=torch.long).to(device)\n",
        "    dep = torch.tensor(dep_index[i], dtype=torch.long).to(device)\n",
        "\n",
        "\n",
        "\n",
        "    _, pred = model.forward(sentence_in, pos, dep)\n",
        "    for j in range(len(output_index[i])):\n",
        "      total += 1\n",
        "      \n",
        "      pred_list.append(pred[j])\n",
        "      truth.append(output_index[i][j])\n",
        "\n",
        "      if pred[j] == output_index[i][j]:\n",
        "        c += 1\n",
        "\n",
        "  accuracy = c/total\n",
        "\n",
        "  return truth, pred_list, accuracy\n"
      ],
      "metadata": {
        "id": "zccR5ELo7NM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "hidden_dim = 250\n",
        "\n",
        "model = BiLSTM_CRF(len(full_word_to_ix), tag_to_ix, embedding_dim, hidden_dim, WV_EMBEDDING_DIM, method='scaled-dot').to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, weight_decay=1e-4)\n",
        "\n",
        "import datetime\n",
        "for epoch in range(2):  \n",
        "    time1 = datetime.datetime.now()\n",
        "    train_loss = 0\n",
        "    method='scaled-dot'\n",
        "    model.train()\n",
        "    for i in range(len(train_input_index)):\n",
        "        #print(i)\n",
        "        tags_index = train_output_index[i]\n",
        "        dep_index = train_dep_index[i]\n",
        "        pos_index = train_pos_index[i]\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is,\n",
        "        # turn them into Tensors of word indices.\n",
        "        sentence_in = torch.tensor(train_input_index[i], dtype=torch.long).to(device)\n",
        "        pos = torch.tensor(pos_index, dtype=torch.long).to(device)\n",
        "        dep = torch.tensor(dep_index, dtype=torch.long).to(device)\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "        # Step 3. Run our forward pass.\n",
        "        loss = model.neg_ll(sentence_in, targets, pos, dep, method='scaled-dot')\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        # calling optimizer.step()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss+=loss.item()\n",
        "\n",
        "    model.eval()\n",
        "     # \n",
        "    _, _, train_acc = accuracy_calculation(model,train_input_index,train_output_index,train_pos_index,train_dep_index)\n",
        "    _, _, val_acc = accuracy_calculation(model,val_input_index,val_output_index,val_pos_index,val_dep_index)\n",
        "\n",
        "    val_loss = 0\n",
        "    for i in range(len(val_input_index)):\n",
        "        tags_index = val_output_index[i]\n",
        "        pos_index = val_pos_index[i]\n",
        "        dep_index = val_dep_index[i]\n",
        "        sentence_in = torch.tensor(val_input_index[i], dtype=torch.long).to(device)\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "        pos = torch.tensor(pos_index, dtype=torch.long).to(device)\n",
        "        dep = torch.tensor(dep_index, dtype=torch.long).to(device)\n",
        "        loss = model.neg_ll(sentence_in, targets, pos, dep, method='scaled-dot')\n",
        "        val_loss+=loss.item()\n",
        "    time2 = datetime.datetime.now()\n",
        "\n",
        "    print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))\n",
        "\n",
        "y_pred, y_true, _ = accuracy_calculation(model,val_input_index,val_output_index,val_pos_index,val_dep_index)\n",
        "def decode_output(output_list):\n",
        "    ix_to_tag = {v:k for k,v in tag_to_ix.items()}\n",
        "    return [ix_to_tag[output] for output in output_list]\n",
        "y_true_decode = decode_output(y_true)\n",
        "y_pred_decode = decode_output(y_pred)\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true_decode,y_pred_decode,digits=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e88ee77-21fb-43b6-a5d2-09336737ab20",
        "id": "5W25hRVi7NM3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:1, Training loss: 24818.84, train acc: 0.9912, val loss: 1663.70, val acc: 0.9886, time: 611.66s\n",
            "Epoch:2, Training loss: 3723.22, train acc: 0.9979, val loss: 1371.56, val acc: 0.9946, time: 619.91s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9707    0.9956    0.9830      1600\n",
            "           D     0.9296    0.9763    0.9524       379\n",
            "           O     0.9997    0.9926    0.9961     19120\n",
            "           P     0.9982    0.9995    0.9989      3931\n",
            "           S     0.9898    0.9994    0.9946      3290\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.9598    0.9853    0.9724      1431\n",
            "\n",
            "    accuracy                         0.9945     33354\n",
            "   macro avg     0.9783    0.9927    0.9853     33354\n",
            "weighted avg     0.9947    0.9945    0.9946     33354\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.metrics import f1_score\n",
        "sklearn.metrics.f1_score(y_true_decode, y_pred_decode,average='micro')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa713f8f-4e35-4053-a5ba-2c068ccf5678",
        "id": "8tqr18MW7NM4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9945433831024765"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline Model"
      ],
      "metadata": {
        "id": "M3X2sDRahRI0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BiLSTM + CRF  + 1 layer <br>\n",
        "Embedding: Fasttext(training+validation data) + pos + dep + fasttext(dota-wiki + CONDA test)"
      ],
      "metadata": {
        "id": "C23Ay_QgpQ0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from numpy.linalg import norm\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "def argmax(vec):\n",
        "    # return the argmax as a python int\n",
        "    _, idx = torch.max(vec, 1)\n",
        "    return idx.item()\n",
        "\n",
        "\n",
        "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
        "def log_sum_exp(vec):\n",
        "    max_score = vec[0, argmax(vec)]\n",
        "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "    return max_score + \\\n",
        "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
        "\n",
        "class BiLSTM_CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim, wv_embedding_dim):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "        self.wv_embedding_dim = wv_embedding_dim\n",
        "        self.word_embeds = nn.Embedding(vocab_size, wv_embedding_dim)\n",
        "        self.word_embeds.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "        \n",
        "        self.pos_embeds = nn.Embedding(pos_embedding.shape[0], pos_embedding.shape[0])\n",
        "        self.pos_embeds.weight.data.copy_(torch.from_numpy(pos_embedding))\n",
        "        \n",
        "        self.dep_embeds = nn.Embedding(dep_embedding.shape[0], dep_embedding.shape[0])\n",
        "        self.dep_embeds.weight.data.copy_(torch.from_numpy(dep_embedding))\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
        "                            num_layers=1, bidirectional=True,dropout=0.5)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
        "\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.hidden_initial()\n",
        "        self.dropout_lstm=nn.Dropout(p=0.5)\n",
        "        \n",
        "    def hidden_initial(self):\n",
        "        return (torch.randn(2, 1, self.hidden_dim // 2).to(device),\n",
        "                torch.randn(2, 1, self.hidden_dim // 2).to(device))\n",
        "\n",
        "    def _forward_alg(self, feats):\n",
        "        # Do the forward algorithm to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "        #print('_forward_alg')\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of\n",
        "                # the previous tag\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to\n",
        "                # next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the\n",
        "                # edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the\n",
        "                # scores.\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "    # method = dot\n",
        "\n",
        "    def lstm_ft_get(self, sentence, pos, dep):\n",
        "        self.hidden = self.hidden_initial()\n",
        "        embeds = torch.cat((self.word_embeds(sentence).view(len(sentence), 1, -1),self.pos_embeds(pos).view(len(pos), 1, -1),self.dep_embeds(dep).view(len(dep), 1, -1)),2)\n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "        lstm_fts = self.hidden2tag(lstm_out)\n",
        "        return lstm_fts\n",
        "\n",
        "\n",
        "    def sentence_scoring(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1).to(device)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long).to(device), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + \\\n",
        "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def viterbi_decoding(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = []  # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                # We don't include the emission scores here because the max\n",
        "                # does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set\n",
        "            # of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "    def neg_ll(self, sentence, tags, pos, dep):\n",
        "        feats = self.lstm_ft_get(sentence, pos, dep)\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self.sentence_scoring(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    #def forward(self, sentence, ent, pos, dep, bert):  # dont confuse this with _forward_alg above.\n",
        "    def forward(self, sentence, pos, dep):\n",
        "        #print('forward')\n",
        "        # Get the emission scores from the BiLSTM\n",
        "\n",
        "        lstm_fts = self.lstm_ft_get(sentence, pos, dep)\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self.viterbi_decoding(lstm_fts)\n",
        "        return score, tag_seq\n",
        "\n",
        "\n",
        "\n",
        "def accuracy_calculation(model, input_index, output_index, pos_index, dep_index):\n",
        "\n",
        "  c, total = 0, 0\n",
        "\n",
        "  pred_list, truth = [], []\n",
        "\n",
        "  #print('accuracy_calculation')\n",
        "  for i in range(len(input_index)):\n",
        "    sentence_in = torch.tensor(input_index[i], dtype=torch.long).to(device)\n",
        "    \n",
        "    pos = torch.tensor(pos_index[i], dtype=torch.long).to(device)\n",
        "    dep = torch.tensor(dep_index[i], dtype=torch.long).to(device)\n",
        "\n",
        "\n",
        "\n",
        "    _, pred = model.forward(sentence_in, pos, dep)\n",
        "    for j in range(len(output_index[i])):\n",
        "      total += 1\n",
        "      \n",
        "      pred_list.append(pred[j])\n",
        "      truth.append(output_index[i][j])\n",
        "\n",
        "      if pred[j] == output_index[i][j]:\n",
        "        c += 1\n",
        "\n",
        "  accuracy = c/total\n",
        "\n",
        "  return truth, pred_list, accuracy"
      ],
      "metadata": {
        "id": "-eN6JTOx6onL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "hidden_dim = 250\n",
        "\n",
        "model = BiLSTM_CRF(len(full_word_to_ix), tag_to_ix, embedding_dim, hidden_dim, WV_EMBEDDING_DIM).to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.12, weight_decay=1e-4)\n",
        "\n",
        "import datetime\n",
        "for epoch in range(2):  \n",
        "    time1 = datetime.datetime.now()\n",
        "    train_loss = 0\n",
        "    model.train()\n",
        "    for i in range(len(train_input_index)):\n",
        "        #print(i)\n",
        "        tags_index = train_output_index[i]\n",
        "        dep_index = train_dep_index[i]\n",
        "        pos_index = train_pos_index[i]\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is,\n",
        "        # turn them into Tensors of word indices.\n",
        "        sentence_in = torch.tensor(train_input_index[i], dtype=torch.long).to(device)\n",
        "        pos = torch.tensor(pos_index, dtype=torch.long).to(device)\n",
        "        dep = torch.tensor(dep_index, dtype=torch.long).to(device)\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "        # Step 3. Run our forward pass.\n",
        "        loss = model.neg_ll(sentence_in, targets, pos, dep)\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        # calling optimizer.step()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss+=loss.item()\n",
        "\n",
        "    model.eval()\n",
        "     # \n",
        "    _, _, train_acc = accuracy_calculation(model,train_input_index,train_output_index,train_pos_index,train_dep_index)\n",
        "    _, _, val_acc = accuracy_calculation(model,val_input_index,val_output_index,val_pos_index,val_dep_index)\n",
        "\n",
        "    val_loss = 0\n",
        "    for i in range(len(val_input_index)):\n",
        "        tags_index = val_output_index[i]\n",
        "        pos_index = val_pos_index[i]\n",
        "        dep_index = val_dep_index[i]\n",
        "        sentence_in = torch.tensor(val_input_index[i], dtype=torch.long).to(device)\n",
        "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "        pos = torch.tensor(pos_index, dtype=torch.long).to(device)\n",
        "        dep = torch.tensor(dep_index, dtype=torch.long).to(device)\n",
        "        loss = model.neg_ll(sentence_in, targets, pos, dep)\n",
        "        val_loss+=loss.item()\n",
        "    time2 = datetime.datetime.now()\n",
        "\n",
        "    print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds()))\n",
        "\n",
        "y_pred, y_true, _ = accuracy_calculation(model,val_input_index,val_output_index,val_pos_index,val_dep_index)\n",
        "def decode_output(output_list):\n",
        "    ix_to_tag = {v:k for k,v in tag_to_ix.items()}\n",
        "    return [ix_to_tag[output] for output in output_list]\n",
        "y_true_decode = decode_output(y_true)\n",
        "y_pred_decode = decode_output(y_pred)\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true_decode,y_pred_decode,digits=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5a75f77-ddc6-4284-c25a-666b8cf341a6",
        "id": "lBEM4PdY6onP"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:1, Training loss: 13779.81, train acc: 0.9962, val loss: 1135.65, val acc: 0.9933, time: 571.77s\n",
            "Epoch:2, Training loss: 1452.23, train acc: 0.9989, val loss: 1138.28, val acc: 0.9952, time: 558.08s\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C     0.9726    0.9981    0.9852      1599\n",
            "           D     0.9497    0.9895    0.9692       382\n",
            "           O     0.9997    0.9931    0.9964     19111\n",
            "           P     0.9987    0.9997    0.9992      3932\n",
            "           S     0.9928    0.9985    0.9956      3303\n",
            "        SEPA     1.0000    1.0000    1.0000      3603\n",
            "           T     0.9639    0.9944    0.9789      1424\n",
            "\n",
            "    accuracy                         0.9954     33354\n",
            "   macro avg     0.9825    0.9962    0.9892     33354\n",
            "weighted avg     0.9956    0.9954    0.9955     33354\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.metrics import f1_score\n",
        "sklearn.metrics.f1_score(y_true_decode, y_pred_decode,average='micro')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a488875-3b7c-4f45-8314-f59e6b8504ed",
        "id": "JoAvHlwO6onQ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9954428254482222"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    }
  ]
}
